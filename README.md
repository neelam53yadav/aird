# PrimeData

**AI-ready data from any source. Ingest, clean, chunk, embed & index. Test and export with confidence.**

PrimeData is a comprehensive enterprise data platform designed for AI workflows. It provides end-to-end data processing, from ingestion to vectorization, with enterprise-grade data quality management, billing & gating, team collaboration, and advanced analytics.

## ğŸš€ Features

### **ğŸ¢ Enterprise Data Quality Management**
- **7 Rule Types**: Required fields, duplicate detection, chunk coverage, bad extensions, file size limits, content validation, and custom rules
- **Real-time Monitoring**: Continuous quality assessment with violation tracking
- **Audit Trail**: Complete history of rule changes and violations
- **Compliance Reporting**: Enterprise-grade governance and compliance features
- **Database-First Architecture**: ACID-compliant rule storage with concurrent access support

### **ğŸ’° Billing & Gating with Stripe**
- **Subscription Plans**: Free, Pro, and Enterprise tiers with usage limits
- **Usage Tracking**: Monitor products, data sources, pipeline runs, and vector storage
- **Stripe Integration**: Secure payment processing with customer portal
- **Plan Limits**: Enforced limits on products, data sources, and pipeline runs
- **Webhook Support**: Real-time subscription updates

### **ğŸ‘¥ Team Management**
- **Role-Based Access**: Owner, Admin, Editor, and Viewer roles
- **Team Invitations**: Invite members with specific roles
- **Workspace Management**: Multi-workspace support with proper access control
- **User Profiles**: Comprehensive user profile management with Google OAuth

### **ğŸ“Š Analytics & Monitoring**
- **Real-time Metrics**: Pipeline performance, success rates, processing times
- **Data Quality Scores**: Continuous quality assessment and reporting
- **Monthly Trends**: Historical performance analysis
- **Activity Tracking**: Recent pipeline runs and system events

### **ğŸ”§ Advanced Chunking Configuration**
- **Hybrid Mode**: Auto and manual chunking with AI-powered optimization
- **Content Analysis**: Detects content types (legal, code, documentation) and suggests optimal settings
- **Smart Defaults**: Intelligent recommendations based on content characteristics
- **Model Optimization**: AI-optimized chunking parameters

### **ğŸ§ª MLflow Integration**
- **Experiment Tracking**: Complete pipeline run tracking with parameters, metrics, and artifacts
- **Performance Monitoring**: Track chunking, embedding, and indexing performance over time
- **Artifact Management**: Store sample chunks, provenance data, and model artifacts
- **UI Integration**: Pipeline metrics dashboard with direct MLflow access

### **ğŸ“¤ Export & Data Management**
- **Export Bundles**: Create downloadable ZIP archives of processed data
- **Data Provenance**: Complete lineage tracking from source to vector
- **Version Management**: Track different versions of processed data
- **Presigned URLs**: Secure, time-limited download links

### **ğŸ”Œ Data Connectors**
- **Web Scraping**: Extract data from websites and online sources
- **Folder Monitoring**: Process files from local and remote directories
- **Database Connectors**: Connect to various database systems
- **API Integrations**: RESTful API data ingestion

### **ğŸ—ï¸ Enterprise Architecture**
- **Microservices**: FastAPI backend with Next.js frontend
- **Vector Storage**: Qdrant for embeddings and similarity search
- **Object Storage**: MinIO for scalable file storage
- **Orchestration**: Airflow-powered data pipelines
- **Database**: PostgreSQL with ACID compliance
- **Authentication**: JWT-based security with Google OAuth

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Node.js 18+**
- **Docker & Docker Compose**
- **Git**

### Setup

#### **Option 1: Automated Setup (Recommended)**

1. **Clone and navigate to the project:**
   ```cmd
   git clone <repository-url>
   cd PrimeData
   ```

2. **Run the complete setup script:**
   ```cmd
   setup_mlflow.bat
   ```
   This will:
   - Activate the virtual environment
   - Install MLflow and all dependencies
   - Test the MLflow integration
   - Provide next steps

#### **Option 2: Manual Setup**

1. **Create virtual environment and install dependencies:**
   ```cmd
   cd PrimeData
   activate_venv.bat
   pip install -r backend\requirements.txt
   install_mlflow.bat
   ```

2. **Start all services:**
   ```cmd
   docker-compose -f infra\docker-compose.yml up -d
   ```

3. **Start MLflow server:**
   ```cmd
   start_mlflow_server.bat
   ```

4. **Start the backend API:**
   ```cmd
   start_backend.bat
   ```

5. **Start the UI:**
   ```cmd
   cd ui
   npm install
   npm run dev
   ```

### **Database Setup**

1. **Run database migrations:**
   ```cmd
   cd backend
   activate_venv.bat
   alembic upgrade head
   ```

### **Service URLs**

- **PrimeData UI**: http://localhost:3000
- **PrimeData API**: http://localhost:8000/health
- **MLflow UI**: http://localhost:5000
- **Airflow UI**: http://localhost:8080
- **MinIO Console**: http://localhost:9001
- **Qdrant Dashboard**: http://localhost:6333

### **Default Credentials**

- **Airflow**: admin / admin
- **MinIO**: minioadmin / minioadmin123
- **PostgreSQL**: primedata / primedata123

## ğŸ“ Project Structure

```
PrimeData/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ src/primedata/         # Python package
â”‚   â”‚   â”œâ”€â”€ api/               # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py        # Authentication & user management
â”‚   â”‚   â”‚   â”œâ”€â”€ products.py    # Product management
â”‚   â”‚   â”‚   â”œâ”€â”€ datasources.py # Data source connectors
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py    # Pipeline orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ data_quality.py # Data quality rules
â”‚   â”‚   â”‚   â”œâ”€â”€ billing.py     # Stripe billing integration
â”‚   â”‚   â”‚   â”œâ”€â”€ analytics.py   # Analytics & metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ exports.py     # Export bundle management
â”‚   â”‚   â”‚   â””â”€â”€ ai_readiness.py # AI readiness assessment
â”‚   â”‚   â”œâ”€â”€ core/              # Core functionality
â”‚   â”‚   â”œâ”€â”€ db/                # Database models and connection
â”‚   â”‚   â”œâ”€â”€ indexing/          # Embedding and vector operations
â”‚   â”‚   â”œâ”€â”€ connectors/        # Data source connectors
â”‚   â”‚   â””â”€â”€ analysis/          # Content analysis and chunking
â”‚   â”œâ”€â”€ alembic/               # Database migrations
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ ui/                        # Next.js frontend
â”‚   â”œâ”€â”€ app/                   # App Router pages
â”‚   â”‚   â”œâ”€â”€ app/products/      # Product management
â”‚   â”‚   â”œâ”€â”€ app/datasources/   # Data source management
â”‚   â”‚   â”œâ”€â”€ app/analytics/     # Analytics dashboard
â”‚   â”‚   â”œâ”€â”€ app/billing/       # Billing & subscription management
â”‚   â”‚   â”œâ”€â”€ app/team/          # Team management
â”‚   â”‚   â”œâ”€â”€ app/settings/      # User settings
â”‚   â”‚   â””â”€â”€ api/               # API routes
â”‚   â”œâ”€â”€ components/            # React components
â”‚   â””â”€â”€ lib/                   # Utilities and API client
â”œâ”€â”€ infra/                     # Infrastructure
â”‚   â”œâ”€â”€ docker-compose.yml     # Service definitions
â”‚   â”œâ”€â”€ airflow/               # Airflow DAGs and configuration
â”‚   â”œâ”€â”€ env/                   # Environment templates
â”‚   â””â”€â”€ init/                  # Initialization scripts
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ architecture.md        # System architecture
â”‚   â”œâ”€â”€ data-quality.md        # Data quality management
â”‚   â”œâ”€â”€ pipeline-guide.md      # Pipeline setup and usage
â”‚   â”œâ”€â”€ api-reference.md       # API documentation
â”‚   â””â”€â”€ troubleshooting/       # Troubleshooting guides
â”œâ”€â”€ *.bat                      # Windows batch scripts for setup
â””â”€â”€ MLFLOW_TROUBLESHOOTING.md  # MLflow troubleshooting guide
```

## ğŸ¯ Usage Guide

### **Getting Started**

1. **Access the UI**: Go to http://localhost:3000
2. **Sign in with Google**: Use your Google account for authentication
3. **Create a Product**: Click "New Product" and fill in details
4. **Configure Chunking**: Choose between Auto or Manual mode
5. **Add Data Sources**: Connect web URLs, folders, or other data sources
6. **Set Data Quality Rules**: Configure validation rules for your data
7. **Run Pipeline**: Execute the data processing pipeline
8. **Monitor Results**: Check analytics dashboard and MLflow UI

### **Core Workflows**

#### **1. Product Management**
- Create and manage data products
- Configure chunking strategies (auto/manual)
- Set up data quality rules
- Monitor pipeline performance

#### **2. Data Source Management**
- Connect web sources, folders, databases
- Configure data extraction settings
- Monitor data freshness and quality
- Manage data source permissions

#### **3. Data Quality Management**
- Define validation rules (7 rule types)
- Monitor quality violations in real-time
- Generate compliance reports
- Track quality trends over time

#### **4. Team Collaboration**
- Invite team members with specific roles
- Manage workspace permissions
- Track team activity and usage
- Configure team-wide settings

#### **5. Billing & Usage**
- Monitor usage against plan limits
- Upgrade/downgrade subscription plans
- Manage payment methods
- Track usage analytics

### **Advanced Features**

#### **Chunking Configuration**
- **Auto Mode**: AI analyzes content and optimizes parameters
- **Manual Mode**: Full control over chunk size, overlap, and strategy
- **Content Analysis**: Detects content types and suggests optimal settings
- **Model Optimization**: AI-powered parameter tuning

#### **MLflow Integration**
- **Experiment Tracking**: Complete pipeline run history
- **Performance Metrics**: Processing time, quality scores, success rates
- **Artifact Management**: Sample chunks, provenance data, model artifacts
- **A/B Testing**: Compare different configurations

#### **Export & Data Management**
- **Export Bundles**: Download processed data as ZIP archives
- **Data Provenance**: Complete lineage from source to vector
- **Version Control**: Track different versions of processed data
- **Secure Downloads**: Presigned URLs for data access

## ğŸ› ï¸ Development

### **Batch Scripts Available**

- `setup_mlflow.bat` - Complete MLflow setup
- `activate_venv.bat` - Activate virtual environment
- `install_mlflow.bat` - Install MLflow dependencies
- `test_mlflow.bat` - Test MLflow integration
- `start_backend.bat` - Start FastAPI server
- `start_mlflow_server.bat` - Start MLflow tracking server
- `rebuild_airflow_with_mlflow.bat` - Rebuild Airflow with MLflow

### **API Documentation**

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### **Database Migrations**
```cmd
cd backend
activate_venv.bat
alembic upgrade head
```

## ğŸ“Š Monitoring & Observability

### **Pipeline Metrics**
- Chunking performance and quality
- Embedding generation speed
- Vector indexing efficiency
- Overall pipeline health and processing time
- Real-time metrics aggregation

### **Analytics Dashboard**
- Product performance overview
- Data quality trends
- Team activity monitoring
- Usage analytics and billing insights

### **MLflow Experiments**
- Historical performance trends
- Configuration impact analysis
- A/B testing capabilities
- Model performance tracking

## ğŸ”§ Configuration

### **Environment Variables**

#### **Backend (.env)**
```env
# Database
DATABASE_URL=postgresql://primedata:primedata123@localhost:5432/primedata

# MLflow
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_BACKEND_STORE_URI=postgresql://primedata:primedata123@localhost:5432/primedata
MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow-artifacts

# MinIO
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123

# Qdrant
QDRANT_URL=http://localhost:6333

# Stripe (for billing)
STRIPE_SECRET_KEY=sk_test_...
STRIPE_WEBHOOK_SECRET=whsec_...
STRIPE_PRO_PRICE_ID=price_...
STRIPE_ENTERPRISE_PRICE_ID=price_...

# Frontend
FRONTEND_URL=http://localhost:3000
```

#### **Frontend (.env.local)**
```env
# NextAuth Configuration
NEXTAUTH_SECRET=your-secret-key
NEXTAUTH_URL=http://localhost:3000

# Google OAuth
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret

# Stripe
NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY=pk_test_...

# API
NEXT_PUBLIC_API_URL=http://localhost:8000
```

## ğŸ“š Documentation

### **Core Documentation**
- **[Architecture Guide](docs/architecture.md)** - System architecture and component overview
- **[Setup Guide](docs/setup-guide.md)** - Complete installation and configuration guide
- **[User Guide](docs/user-guide.md)** - Comprehensive user manual and best practices
- **[Features Overview](docs/features-overview.md)** - Complete feature overview and capabilities
- **[API Reference](docs/api-reference.md)** - Complete API documentation and examples
- **[Data Quality Management](docs/data-quality.md)** - Enterprise data quality rules and management
- **[Pipeline Guide](docs/pipeline-guide.md)** - Complete pipeline setup and usage
- **[AI Readiness](docs/ai-readiness.md)** - Data quality assessment and improvement

### **Troubleshooting & Support**
- **[Troubleshooting Guide](docs/troubleshooting.md)** - Comprehensive troubleshooting for all issues
- **[FAQ](docs/faq.md)** - Frequently asked questions and answers
- **[MLflow Troubleshooting](MLFLOW_TROUBLESHOOTING.md)** - MLflow setup and common issues
- **[Pipeline Troubleshooting](docs/pipeline-troubleshooting.md)** - Airflow and pipeline issues
- **[Data Quality Troubleshooting](docs/data-quality-troubleshooting.md)** - Data quality rule problems

### **When to Use Which Documentation**

#### **Getting Started**
1. **First Time Setup**: Start with this README â†’ Setup Guide â†’ User Guide
2. **Understanding the System**: Architecture â†’ Data Quality â†’ AI Readiness
3. **Setting Up Pipelines**: Pipeline Guide â†’ API Reference

#### **Development & Configuration**
1. **API Development**: API Reference â†’ Architecture
2. **Data Quality Rules**: Data Quality â†’ API Reference
3. **Pipeline Customization**: Pipeline Guide â†’ Architecture
4. **Billing Integration**: API Reference â†’ Architecture

#### **Troubleshooting**
1. **General Issues**: Troubleshooting Guide
2. **MLflow Issues**: MLflow Troubleshooting
3. **Pipeline Failures**: Pipeline Troubleshooting
4. **Data Quality Problems**: Data Quality Troubleshooting

#### **Enterprise Features**
1. **Data Quality Management**: Data Quality â†’ API Reference
2. **Billing & Subscriptions**: API Reference â†’ Architecture
3. **Team Management**: User Guide â†’ API Reference
4. **Compliance & Governance**: Data Quality â†’ Architecture

#### **User Experience**
1. **New Users**: User Guide â†’ Setup Guide
2. **Advanced Users**: API Reference â†’ Architecture
3. **Administrators**: User Guide â†’ Troubleshooting Guide
4. **Developers**: API Reference â†’ Pipeline Guide

## ğŸš€ What's New

### **Latest Features**
- âœ… **Enterprise Data Quality Management** - 7 rule types with real-time monitoring
- âœ… **Billing & Gating with Stripe** - Subscription plans with usage limits
- âœ… **Team Management** - Role-based access control and collaboration
- âœ… **Analytics Dashboard** - Real-time metrics and performance monitoring
- âœ… **Export Bundles** - Secure data export with provenance tracking
- âœ… **User Profile Management** - Comprehensive user settings and preferences
- âœ… **Database-First Architecture** - ACID-compliant data quality rules
- âœ… **Hybrid Chunking Configuration** - Auto and manual modes with AI optimization
- âœ… **MLflow Integration** - Complete experiment tracking with accurate metrics
- âœ… **Content Analysis** - AI-powered chunking optimization
- âœ… **Pipeline Metrics Dashboard** - Real-time performance monitoring

### **Upcoming Features**
- ğŸ”„ Advanced content type detection
- ğŸ”„ Custom embedding models
- ğŸ”„ Pipeline scheduling and automation
- ğŸ”„ Advanced analytics and reporting
- ğŸ”„ Multi-workspace data sharing
- ğŸ”„ Custom data quality rules

## ğŸ†˜ Support & Troubleshooting

### **Common Issues**

#### **Setup Issues**
- **Virtual Environment**: Ensure Python 3.11+ is installed
- **Docker Services**: Check all services are running with `docker-compose ps`
- **Database Connection**: Verify PostgreSQL is accessible
- **Port Conflicts**: Ensure ports 3000, 8000, 5000, 8080, 9000, 6333 are available

#### **Authentication Issues**
- **Google OAuth**: Verify client ID and secret are correct
- **JWT Tokens**: Check token expiration and refresh
- **Session Management**: Clear browser cache and cookies

#### **Pipeline Issues**
- **Airflow Connection**: Check Airflow UI at http://localhost:8080
- **MLflow Tracking**: Verify MLflow server is running
- **Data Quality Rules**: Check rule configuration and validation

### **Getting Help**

1. **Check Documentation**: Start with relevant troubleshooting guide
2. **Review Logs**: Check backend logs for error details
3. **Health Check**: Use http://localhost:8000/health for system status
4. **Service Status**: Verify all services are running and accessible

## ğŸ“„ License

Enterprise License - All Rights Reserved

---

**PrimeData** - Transforming data into AI-ready insights with enterprise-grade quality and governance.