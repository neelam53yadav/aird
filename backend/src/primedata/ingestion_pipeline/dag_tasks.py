"""
Airflow DAG Task Functions - Modular Task Definitions

This module contains all task functions for Airflow DAGs.
Following enterprise best practices:
- Separation of concerns: DAG orchestration vs business logic
- Modularity: Task functions are reusable and testable
- Maintainability: Business logic in one place, DAG files stay minimal
"""

import json
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from uuid import UUID

from primedata.db.database import get_db
from primedata.db.models import (
    ArtifactStatus,
    ArtifactType,
    PipelineArtifact,
    PipelineRun,
    PolicyStatus,
    Product,
    RawFile,
    RawFileStatus,
    RetentionPolicy,
)

# NOTE: AIRD stage imports are done lazily inside functions to avoid DAG import timeouts
# Airflow has a 30s timeout for DAG imports, and importing all stages at module level
# causes heavy import chains (embedding_config, sentence_transformers, etc.) that exceed this limit.
# Import only lightweight enums at module level.
from primedata.ingestion_pipeline.aird_stages.base import StageStatus
from primedata.ingestion_pipeline.artifact_registry import (
    calculate_checksum,
    get_artifact_summary_for_run,
    get_artifacts_by_stage,
    register_artifact,
)
from primedata.storage.minio_client import minio_client
from primedata.storage.paths import raw_prefix
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)
std_logger = logging.getLogger(__name__)  # For Airflow compatibility


def mark_raw_files_as_failed(product_id: UUID, version: int, error_message: str, db_session=None) -> int:
    """
    Mark all PROCESSING raw files for a product/version as FAILED.

    This helper can be called from any task to mark files as failed when pipeline fails.

    Args:
        product_id: Product UUID
        version: Version number
        error_message: Error message to store
        db_session: Database session (will create new one if None)

    Returns:
        Number of files marked as FAILED
    """
    if db_session is None:
        db = next(get_db())
        should_close = True
    else:
        db = db_session
        should_close = False

    try:
        raw_files_to_fail = (
            db.query(RawFile)
            .filter(RawFile.product_id == product_id, RawFile.version == version, RawFile.status == RawFileStatus.PROCESSING)
            .all()
        )

        if raw_files_to_fail:
            for record in raw_files_to_fail:
                record.status = RawFileStatus.FAILED
                record.error_message = error_message
            db.commit()
            logger.info(f"Marked {len(raw_files_to_fail)} raw files as FAILED for product {product_id}, version {version}")
            std_logger.info(f"Marked {len(raw_files_to_fail)} raw files as FAILED for product {product_id}, version {version}")
            return len(raw_files_to_fail)
        return 0
    except Exception as e:
        logger.error(f"Failed to mark raw files as FAILED: {e}")
        std_logger.error(f"Failed to mark raw files as FAILED: {e}")
        db.rollback()
        return 0
    finally:
        if should_close:
            db.close()


def register_stage_artifacts(
    db: Session,
    pipeline_run_id: UUID,
    workspace_id: UUID,
    product_id: UUID,
    version: int,
    stage_name: str,
    result: Any,  # StageResult
    storage: Any,  # AirdStorageAdapter
    input_artifact_ids: Optional[List[UUID]] = None,
) -> List[UUID]:
    """
    Register artifacts generated by a stage.

    Phase 1 & 2: Registers artifacts with lineage tracking.

    Args:
        db: Database session
        pipeline_run_id: Pipeline run ID
        workspace_id: Workspace ID
        product_id: Product ID
        version: Product version
        stage_name: Stage name
        result: StageResult from stage execution
        storage: AirdStorageAdapter to get artifact info
        input_artifact_ids: List of input artifact IDs for lineage

    Returns:
        List of registered artifact IDs
    """
    if result.status != StageStatus.SUCCEEDED:
        return []  # Don't register artifacts for failed stages

    registered_ids = []

    # Get artifacts from stage result
    artifacts = result.artifacts or {}
    if not artifacts:
        # Try to infer artifacts from stage result metrics
        # Each stage stores artifacts differently, so we need stage-specific logic
        pass

    # Stage-specific artifact registration
    if stage_name == "preprocess":
        # Preprocess generates processed JSONL files
        processed_files = result.metrics.get("processed_file_list", [])
        for file_stem in processed_files:
            # Construct MinIO key for processed JSONL
            storage_key = f"ws/{workspace_id}/prod/{product_id}/v/{version}/clean/{file_stem}.jsonl"

            # Get file info using unified stat_object method
            try:
                from primedata.storage.minio_client import minio_client

                stat_info = minio_client.stat_object("primedata-clean", storage_key)
                if not stat_info:
                    logger.warning(f"Could not get file info for {storage_key}")
                    continue

                file_size = stat_info["size"]
                storage_etag = stat_info["etag"]

                # Calculate checksum from file content
                file_data = minio_client.get_bytes("primedata-clean", storage_key)
                if not file_data:
                    logger.warning(f"Could not download file for checksum calculation: {storage_key}")
                    continue

                checksum = calculate_checksum(file_data, algorithm="sha256")
            except Exception as e:
                logger.warning(f"Could not get file info for {storage_key}: {e}")
                continue

            artifact_id = register_artifact(
                db=db,
                pipeline_run_id=pipeline_run_id,
                workspace_id=workspace_id,
                product_id=product_id,
                version=version,
                stage_name=stage_name,
                artifact_type=ArtifactType.JSONL,
                artifact_name=f"processed_chunks_{file_stem}",
                storage_bucket="primedata-clean",
                storage_key=storage_key,
                file_size=file_size,
                checksum=checksum,
                storage_etag=storage_etag,
                input_artifact_ids=input_artifact_ids,  # Would be raw file artifact IDs
                artifact_metadata={
                    "file_stem": file_stem,
                    "chunks_count": result.metrics.get("file_chunk_counts", {}).get(
                        file_stem, result.metrics.get("total_chunks", 0)
                    ),
                    "playbook_id": result.metrics.get("playbook_id"),
                },
                retention_policy=RetentionPolicy.DAYS_90,
            ).id
            registered_ids.append(artifact_id)

    elif stage_name == "scoring":
        # Scoring generates metrics JSON
        # Use clean_prefix to match where storage.put_metrics_json() stores it
        from primedata.storage.paths import clean_prefix

        metrics_key = f"{clean_prefix(workspace_id, product_id, version)}metrics.json"
        try:
            from primedata.storage.minio_client import minio_client

            stat_info = minio_client.stat_object("primedata-clean", metrics_key)
            if not stat_info:
                logger.warning(f"Could not get metrics.json info")
            else:
                file_size = stat_info["size"]
                storage_etag = stat_info["etag"]

                # Calculate checksum from file content
                file_data = minio_client.get_bytes("primedata-clean", metrics_key)
                if not file_data:
                    logger.warning(f"Could not download metrics.json for checksum calculation")
                else:
                    checksum = calculate_checksum(file_data, algorithm="sha256")

                    artifact_id = register_artifact(
                        db=db,
                        pipeline_run_id=pipeline_run_id,
                        workspace_id=workspace_id,
                        product_id=product_id,
                        version=version,
                        stage_name=stage_name,
                        artifact_type=ArtifactType.JSON,
                        artifact_name="metrics",
                        storage_bucket="primedata-clean",
                        storage_key=metrics_key,
                        file_size=file_size,
                        checksum=checksum,
                        storage_etag=storage_etag,
                        input_artifact_ids=input_artifact_ids,  # Would be preprocess artifact IDs
                        artifact_metadata={
                            "total_chunks": result.metrics.get("total_chunks", 0),
                            "avg_trust_score": result.metrics.get("avg_trust_score", 0.0),
                        },
                        retention_policy=RetentionPolicy.DAYS_90,
                    ).id
                    registered_ids.append(artifact_id)
        except Exception as e:
            logger.warning(f"Could not get metrics.json info: {e}")

    elif stage_name == "fingerprint":
        # Fingerprint is stored via storage.put_artifact() in primedata-exports under artifacts/
        # Path format: ws/{workspace_id}/prod/{product_id}/v/{version}/artifacts/fingerprint.json
        fingerprint_key = f"ws/{workspace_id}/prod/{product_id}/v/{version}/artifacts/fingerprint.json"
        try:
            from primedata.storage.minio_client import minio_client

            stat_info = minio_client.stat_object("primedata-exports", fingerprint_key)
            if not stat_info:
                logger.warning(f"Could not get fingerprint.json info")
            else:
                file_size = stat_info["size"]
                storage_etag = stat_info["etag"]

                # Calculate checksum from file content
                file_data = minio_client.get_bytes("primedata-exports", fingerprint_key)
                if not file_data:
                    logger.warning(f"Could not download fingerprint.json for checksum calculation")
                else:
                    checksum = calculate_checksum(file_data, algorithm="sha256")

                    artifact_id = register_artifact(
                        db=db,
                        pipeline_run_id=pipeline_run_id,
                        workspace_id=workspace_id,
                        product_id=product_id,
                        version=version,
                        stage_name=stage_name,
                        artifact_type=ArtifactType.JSON,
                        artifact_name="fingerprint",
                        storage_bucket="primedata-exports",
                        storage_key=fingerprint_key,
                        file_size=file_size,
                        checksum=checksum,
                        storage_etag=storage_etag,
                        input_artifact_ids=input_artifact_ids,  # scoring artifacts
                        artifact_metadata=result.metrics.get("fingerprint", {}),
                        retention_policy=RetentionPolicy.KEEP_FOREVER,  # Fingerprints are important
                    ).id
                    registered_ids.append(artifact_id)
        except Exception as e:
            logger.warning(f"Could not get fingerprint.json info: {e}")

    elif stage_name == "validation":
        # Validation generates CSV summary via storage.put_artifact -> primedata-exports ws/.../artifacts/ai_validation_summary.csv
        csv_key = (result.artifacts.get("validation_summary_csv") if result.artifacts else None) or result.metrics.get(
            "validation_summary_path"
        )
        if csv_key:
            bucket = "primedata-exports"
            key = csv_key
            try:
                from primedata.storage.minio_client import minio_client

                stat_info = minio_client.stat_object(bucket, key)
                if not stat_info:
                    logger.warning(f"Could not get validation CSV info")
                else:
                    file_size = stat_info["size"]
                    storage_etag = stat_info["etag"]

                    # Calculate checksum from file content
                    file_data = minio_client.get_bytes(bucket, key)
                    if not file_data:
                        logger.warning(f"Could not download validation CSV for checksum calculation")
                    else:
                        checksum = calculate_checksum(file_data, algorithm="sha256")

                        artifact_id = register_artifact(
                            db=db,
                            pipeline_run_id=pipeline_run_id,
                            workspace_id=workspace_id,
                            product_id=product_id,
                            version=version,
                            stage_name=stage_name,
                            artifact_type=ArtifactType.CSV,
                            artifact_name="validation_summary",
                            storage_bucket=bucket,
                            storage_key=key,
                            file_size=file_size,
                            checksum=checksum,
                            storage_etag=storage_etag,
                            input_artifact_ids=input_artifact_ids,
                            artifact_metadata={
                                "threshold": result.metrics.get("threshold", 70.0),
                            },
                            retention_policy=RetentionPolicy.DAYS_90,
                        ).id
                        registered_ids.append(artifact_id)
            except Exception as e:
                logger.warning(f"Could not get validation CSV info: {e}")

    elif stage_name == "reporting":
        # Reporting stores PDF via storage.put_artifact -> primedata-exports ws/.../artifacts/ai_trust_report.pdf
        pdf_key = (result.artifacts.get("trust_report_pdf") if result.artifacts else None) or result.metrics.get(
            "trust_report_path"
        )
        if pdf_key:
            bucket = "primedata-exports"
            key = pdf_key

            try:
                from primedata.storage.minio_client import minio_client

                stat_info = minio_client.stat_object(bucket, key)
                if not stat_info:
                    logger.warning(f"Could not get trust report PDF info")
                else:
                    file_size = stat_info["size"]
                    storage_etag = stat_info["etag"]

                    # Calculate checksum from file content
                    file_data = minio_client.get_bytes(bucket, key)
                    if not file_data:
                        logger.warning(f"Could not download trust report PDF for checksum calculation")
                    else:
                        checksum = calculate_checksum(file_data, algorithm="sha256")

                        artifact_id = register_artifact(
                            db=db,
                            pipeline_run_id=pipeline_run_id,
                            workspace_id=workspace_id,
                            product_id=product_id,
                            version=version,
                            stage_name=stage_name,
                            artifact_type=ArtifactType.PDF,
                            artifact_name="trust_report",
                            storage_bucket=bucket,
                            storage_key=key,
                            file_size=file_size,
                            checksum=checksum,
                            storage_etag=storage_etag,
                            input_artifact_ids=input_artifact_ids,
                            artifact_metadata={
                                "threshold": result.metrics.get("threshold", 70.0),
                                "pdf_size_bytes": result.metrics.get("pdf_size_bytes"),
                            },
                            retention_policy=RetentionPolicy.KEEP_FOREVER,  # Reports are important
                        ).id
                        registered_ids.append(artifact_id)
            except Exception as e:
                logger.warning(f"Could not get trust report PDF info: {e}")

    elif stage_name == "indexing":
        # Indexing creates vectors in Qdrant (not stored in MinIO, but we track it)
        # Prepare artifact metadata
        artifact_metadata_dict = {
            "vectors_indexed": result.metrics.get("points_indexed", 0),  # Indexing stage uses 'points_indexed'
            "collection_name": result.metrics.get("collection_name"),
            "embedding_model": result.metrics.get("embedding_model"),
        }

        # Calculate checksum from metadata JSON (since vectors are in Qdrant, not MinIO)
        metadata_json = json.dumps(artifact_metadata_dict, sort_keys=True)
        metadata_bytes = metadata_json.encode("utf-8")
        metadata_checksum = calculate_checksum(metadata_bytes, algorithm="sha256")

        artifact_id = register_artifact(
            db=db,
            pipeline_run_id=pipeline_run_id,
            workspace_id=workspace_id,
            product_id=product_id,
            version=version,
            stage_name=stage_name,
            artifact_type=ArtifactType.VECTOR,
            artifact_name="qdrant_vectors",
            storage_bucket="qdrant",  # Special bucket name for Qdrant
            storage_key=f"collection_ws_{workspace_id}_prod_{product_id}_v_{version}",
            file_size=0,  # Vectors are in Qdrant, not MinIO
            checksum=metadata_checksum,  # Calculate checksum from metadata JSON
            storage_etag=metadata_checksum,  # Use checksum as ETag since there's no file
            input_artifact_ids=input_artifact_ids,
            artifact_metadata=artifact_metadata_dict,
            retention_policy=RetentionPolicy.KEEP_FOREVER,  # Vectors are critical
        ).id
        registered_ids.append(artifact_id)

    logger.info(f"Registered {len(registered_ids)} artifacts for stage {stage_name}")
    return registered_ids


def raise_if_stage_failed(result, stage_name: str, context_msg: str = ""):
    """
    Raise RuntimeError if stage result indicates failure.

    Airflow PythonOperator tasks only fail when exceptions are raised.
    This helper ensures that failed stage results cause task failures.

    Args:
        result: StageResult object from stage execution
        stage_name: Name of the stage (for error messages)
        context_msg: Additional context message to include in error
    """
    if result.status == StageStatus.FAILED:
        error_msg = result.error or "Unknown error"
        full_msg = f"{stage_name} stage failed: {error_msg}"
        if context_msg:
            full_msg += f" ({context_msg})"

        logger.error(f"‚ùå {full_msg}")
        logger.error(f"Stage metrics: {result.metrics}")
        logger.error(f"Stage started_at: {result.started_at}, finished_at: {result.finished_at}")

        raise RuntimeError(full_msg)


def get_dag_params(**context) -> Dict[str, Any]:
    """Extract and validate DAG parameters from Airflow context.

    Args:
        context: Airflow task context

    Returns:
        Dictionary with validated parameters (workspace_id, product_id, version, etc.)
    """
    logger.info(f"Context keys: {list(context.keys())}")

    # Try to get parameters from DAG run conf first, then fall back to params
    dag_run = context.get("dag_run")
    if dag_run:
        logger.info(f"DAG run found: {dag_run.run_id}")
        if dag_run.conf:
            params = dag_run.conf
            logger.info(f"Using DAG run conf parameters: {params}")
        else:
            logger.warning("DAG run conf is empty")
            params = context.get("params", {})
            logger.info(f"Using default params: {params}")
    else:
        logger.warning("No DAG run found in context")
        params = context.get("params", {})
        logger.info(f"Using default params: {params}")

    # Get required parameters
    workspace_id = params.get("workspace_id")
    product_id = params.get("product_id")
    version = params.get("version")
    playbook_id = params.get("playbook_id")

    # Get embedding configuration
    embedding_config = params.get("embedding_config", {})
    embedder_name = embedding_config.get("embedder_name", "minilm")
    dim = int(embedding_config.get("embedding_dimension", 384))

    # Get chunking configuration
    chunking_config = params.get("chunking_config", {})

    logger.info(
        f"Extracted parameters: workspace_id={workspace_id}, product_id={product_id}, version={version}, playbook_id={playbook_id}, embedder={embedder_name}, dim={dim}"
    )
    logger.info(f"Chunking config: {chunking_config}")

    if not product_id:
        error_msg = f"product_id parameter is required but was None. Available params: {params}. DAG run: {dag_run.run_id if dag_run else 'None'}"
        logger.error(error_msg)
        raise ValueError(error_msg)

    logger.info(
        f"Pipeline parameters validated: workspace_id={workspace_id}, product_id={product_id}, version={version}, embedder={embedder_name}, dim={dim}"
    )

    return {
        "workspace_id": UUID(workspace_id) if isinstance(workspace_id, str) else workspace_id,
        "product_id": UUID(product_id) if isinstance(product_id, str) else product_id,
        "version": version,
        "playbook_id": playbook_id,
        "embedder_name": embedder_name,
        "dim": dim,
        "chunking_config": chunking_config,
    }


def get_aird_context(**context) -> Dict[str, Any]:
    """Get AIRD stage execution context.

    This helper function provides context for AIRD stages including:
    - Pipeline parameters
    - Storage adapter
    - Database session
    - Pipeline run tracking

    Args:
        context: Airflow task context

    Returns:
        Dictionary with AIRD context (storage, tracker, config, etc.)
    """
    # Lazy imports to avoid DAG import timeouts (Airflow has 30s limit)
    from primedata.ingestion_pipeline.aird_stages.config import get_aird_config
    from primedata.ingestion_pipeline.aird_stages.storage import AirdStorageAdapter
    from primedata.ingestion_pipeline.aird_stages.tracking import StageTracker

    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]

    # Get database session
    db = next(get_db())

    # Get pipeline run
    pipeline_run = db.query(PipelineRun).filter(PipelineRun.product_id == product_id, PipelineRun.version == version).first()

    if not pipeline_run:
        logger.warning(f"Pipeline run not found for product {product_id}, version {version}")
        pipeline_run = None

    # Create storage adapter
    storage = AirdStorageAdapter(
        workspace_id=workspace_id,
        product_id=product_id,
        version=version,
    )

    # Create stage tracker (if pipeline run exists)
    tracker = None
    if pipeline_run:
        tracker = StageTracker(db, pipeline_run)

    # Get AIRD config
    aird_config = get_aird_config()

    return {
        "workspace_id": workspace_id,
        "product_id": product_id,
        "version": version,
        "storage": storage,
        "tracker": tracker,
        "db": db,
        "pipeline_run": pipeline_run,
        "config": aird_config,
        "playbook_id": params.get("playbook_id"),
    }


def update_pipeline_status(pipeline_run_id: str, status: str, metrics: Dict[str, Any] = None):
    """Update pipeline run status in the database via API."""
    try:
        import requests

        # Get the backend URL from environment or use default
        backend_url = os.getenv("PRIMEDATA_BACKEND_URL", "http://host.docker.internal:8000")

        # Update the pipeline run status
        update_data = {
            "status": status,
            "finished_at": datetime.utcnow().isoformat() if status in ["succeeded", "failed"] else None,
            "metrics": metrics or {},
        }

        response = requests.patch(
            f"{backend_url}/api/v1/pipeline/runs/{pipeline_run_id}",
            json=update_data,
            headers={"Content-Type": "application/json"},
            timeout=30,
        )

        if response.status_code == 200:
            logger.info(f"Successfully updated pipeline run {pipeline_run_id} to status {status}")
        else:
            logger.error(f"Failed to update pipeline run {pipeline_run_id}: {response.status_code} - {response.text}")

    except Exception as e:
        logger.error(f"Error updating pipeline run {pipeline_run_id}: {e}")


def get_pipeline_run_id_from_context(**context) -> str:
    """Extract pipeline run ID from DAG run parameters."""
    try:
        # Try to get from DAG run conf
        dag_run = context.get("dag_run")
        if dag_run and dag_run.conf:
            return dag_run.conf.get("pipeline_run_id")

        # Try to get from task instance
        task_instance = context.get("task_instance")
        if task_instance and task_instance.dag_run:
            return task_instance.dag_run.conf.get("pipeline_run_id")

        # Try to get from params
        params = context.get("params", {})
        return params.get("pipeline_run_id")

    except Exception as e:
        logger.error(f"Error extracting pipeline run ID: {e}")
        return None


def sample_files_for_analysis(
    raw_file_records: List[RawFile], max_files: int = 5, max_per_datasource: int = 2
) -> List[RawFile]:
    """
    Sample representative files from raw file records for content analysis.
    
    Strategy:
    - Group files by data_source_id to ensure representation across datasources
    - Randomly select 1-2 files per datasource (up to max_files total)
    - Prioritize files with diverse extensions/types for better analysis
    
    Args:
        raw_file_records: List of RawFile records to sample from
        max_files: Maximum total files to sample (default: 5)
        max_per_datasource: Maximum files per datasource (default: 2)
    
    Returns:
        List of sampled RawFile records
    """
    import random
    
    if not raw_file_records:
        return []
    
    # Group files by data_source_id (None is treated as a single group)
    files_by_datasource: Dict[Optional[UUID], List[RawFile]] = {}
    for record in raw_file_records:
        ds_id = record.data_source_id
        if ds_id not in files_by_datasource:
            files_by_datasource[ds_id] = []
        files_by_datasource[ds_id].append(record)
    
    logger.info(f"Grouped {len(raw_file_records)} files into {len(files_by_datasource)} datasource(s)")
    
    # Sample files: up to max_per_datasource per datasource, up to max_files total
    sampled_files: List[RawFile] = []
    
    # Shuffle datasources to randomize selection order
    datasource_ids = list(files_by_datasource.keys())
    random.shuffle(datasource_ids)
    
    for ds_id in datasource_ids:
        if len(sampled_files) >= max_files:
            break
        
        files_in_datasource = files_by_datasource[ds_id]
        
        # Prioritize files with diverse extensions for better analysis
        # Sort by extension diversity (prefer .pdf, .txt, .html, .docx, etc.)
        def extension_diversity_key(record: RawFile) -> int:
            ext = Path(record.filename).suffix.lower()
            # Priority order: pdf > txt > html > docx > others
            priority_exts = {'.pdf': 5, '.txt': 4, '.html': 3, '.htm': 3, '.docx': 2, '.doc': 2}
            return priority_exts.get(ext, 1)
        
        files_in_datasource_sorted = sorted(files_in_datasource, key=extension_diversity_key, reverse=True)
        
        # Randomly select up to max_per_datasource files from this datasource
        num_to_sample = min(max_per_datasource, len(files_in_datasource_sorted), max_files - len(sampled_files))
        if num_to_sample > 0:
            # Randomly shuffle and take first num_to_sample
            random.shuffle(files_in_datasource_sorted)
            sampled_files.extend(files_in_datasource_sorted[:num_to_sample])
    
    logger.info(f"Sampled {len(sampled_files)} files from {len(raw_file_records)} total files across {len(files_by_datasource)} datasource(s)")
    for sampled in sampled_files:
        logger.info(f"  - {sampled.filename} (datasource: {sampled.data_source_id}, extension: {Path(sampled.filename).suffix})")
    
    return sampled_files


def extract_text_sample_from_file(
    storage: Any, raw_file: RawFile, max_chars: int = 5000
) -> Optional[str]:
    """
    Extract text sample from a raw file for analysis.
    
    Args:
        storage: AirdStorageAdapter instance
        raw_file: RawFile record
        max_chars: Maximum characters to extract (default: 5000)
    
    Returns:
        Text sample or None if extraction fails
    """
    try:
        filename = raw_file.filename
        file_stem = raw_file.file_stem
        storage_key = raw_file.storage_key
        storage_bucket = raw_file.storage_bucket
        
        # Extract text based on file type
        if filename.lower().endswith('.pdf'):
            # For PDFs, try to extract first 2-3 pages (roughly 2000-5000 chars)
            try:
                text = storage.get_raw_text(file_stem, minio_key=storage_key, minio_bucket=storage_bucket)
                if text:
                    return text[:max_chars]
            except Exception as e:
                logger.warning(f"Failed to extract text from PDF {filename}: {e}")
                return None
        else:
            # For text files, read directly
            try:
                text = storage.get_raw_text(file_stem, minio_key=storage_key, minio_bucket=storage_bucket)
                if text:
                    return text[:max_chars]
            except Exception as e:
                logger.warning(f"Failed to extract text from {filename}: {e}")
                return None
        
        return None
    except Exception as e:
        logger.warning(f"Error extracting text sample from {raw_file.filename}: {e}")
        return None


def auto_detect_playbook_and_chunking(
    product: Product,
    raw_file_records: List[RawFile],
    storage: Any,
    db: Session,
) -> Dict[str, Any]:
    """
    Auto-detect playbook and chunking configuration from sampled files.
    
    Args:
        product: Product instance to update
        raw_file_records: List of RawFile records to sample from
        storage: AirdStorageAdapter instance for reading files
        db: Database session
    
    Returns:
        Dict with detected playbook_id and chunking_config updates, or empty dict if no detection needed
    """
    from collections import Counter
    from primedata.ingestion_pipeline.aird_stages.playbooks import route_playbook
    from primedata.analysis.content_analyzer import content_analyzer
    
    updates = {}
    needs_playbook_detection = product.playbook_id is None
    needs_chunking_detection = (
        product.chunking_config
        and isinstance(product.chunking_config, dict)
        and product.chunking_config.get("mode") == "auto"
    )
    
    if not needs_playbook_detection and not needs_chunking_detection:
        logger.info("No auto-detection needed: playbook_id is set and chunking mode is not auto")
        return updates
    
    if not raw_file_records:
        logger.warning("No raw files available for auto-detection")
        return updates
    
    # Sample files for analysis
    sampled_files = sample_files_for_analysis(raw_file_records, max_files=5, max_per_datasource=2)
    if not sampled_files:
        logger.warning("No files sampled for auto-detection")
        return updates
    
    # Collect text samples and analyze
    playbook_detections = []
    chunking_analyses = []
    sample_files_analyzed = []
    
    for raw_file in sampled_files:
        try:
            # Extract text sample (2000-5000 chars for analysis)
            text_sample = extract_text_sample_from_file(storage, raw_file, max_chars=5000)
            
            if not text_sample or len(text_sample.strip()) < 100:
                logger.warning(f"Skipping {raw_file.filename}: insufficient text content ({len(text_sample or '')} chars)")
                continue
            
            sample_files_analyzed.append({
                "filename": raw_file.filename,
                "file_stem": raw_file.file_stem,
                "data_source_id": str(raw_file.data_source_id) if raw_file.data_source_id else None,
                "chars_extracted": len(text_sample),
            })
            
            # Auto-detect playbook if needed
            if needs_playbook_detection:
                try:
                    playbook_id, reason = route_playbook(sample_text=text_sample[:2000], filename=raw_file.filename)
                    playbook_detections.append((playbook_id, reason))
                    logger.info(f"Detected playbook for {raw_file.filename}: {playbook_id} ({reason})")
                except Exception as e:
                    logger.warning(f"Failed to detect playbook for {raw_file.filename}: {e}")
            
            # Auto-detect chunking config if needed
            if needs_chunking_detection:
                try:
                    chunking_config = content_analyzer.analyze_content(content=text_sample, filename=raw_file.filename)
                    chunking_analyses.append(chunking_config)
                    logger.info(
                        f"Analyzed chunking for {raw_file.filename}: "
                        f"content_type={chunking_config.content_type.value}, "
                        f"confidence={chunking_config.confidence:.2f}, "
                        f"chunk_size={chunking_config.chunk_size}"
                    )
                except Exception as e:
                    logger.warning(f"Failed to analyze chunking for {raw_file.filename}: {e}")
        
        except Exception as e:
            logger.warning(f"Error processing {raw_file.filename} for auto-detection: {e}")
            continue
    
    # Determine most common/relevant playbook
    if needs_playbook_detection and playbook_detections:
        playbook_counts = Counter([pb_id for pb_id, _ in playbook_detections])
        most_common_playbook, count = playbook_counts.most_common(1)[0]
        total_detections = len(playbook_detections)
        
        # Get reason from most common playbook
        most_common_reason = next((reason for pb_id, reason in playbook_detections if pb_id == most_common_playbook), "auto_detected")
        
        logger.info(f"Auto-detected playbook: {most_common_playbook} ({count}/{total_detections} files, reason: {most_common_reason})")
        
        updates["playbook_id"] = most_common_playbook
        updates["playbook_selection"] = {
            "method": "auto_detected",
            "playbook_id": most_common_playbook,
            "reason": most_common_reason,
            "detected_at": datetime.utcnow().isoformat() + "Z",
            "confidence": count / total_detections if total_detections > 0 else 0.0,
            "files_analyzed": len(sample_files_analyzed),
        }
    
    # Determine optimal chunking configuration
    if needs_chunking_detection and chunking_analyses:
        # Use the most confident analysis, or aggregate if multiple
        if len(chunking_analyses) == 1:
            best_analysis = chunking_analyses[0]
        else:
            # Aggregate: use most common content type, average chunk sizes, highest confidence
            content_type_counts = Counter([a.content_type.value for a in chunking_analyses])
            most_common_content_type = content_type_counts.most_common(1)[0][0]
            
            # Find analysis with most common content type and highest confidence
            best_analysis = max(
                [a for a in chunking_analyses if a.content_type.value == most_common_content_type],
                key=lambda a: a.confidence,
                default=chunking_analyses[0],
            )
        
        logger.info(
            f"Auto-detected chunking config: content_type={best_analysis.content_type.value}, "
            f"confidence={best_analysis.confidence:.2f}, "
            f"chunk_size={best_analysis.chunk_size}, "
            f"chunk_overlap={best_analysis.chunk_overlap}"
        )
        
        # Update chunking_config with resolved_settings
        # Preserve existing auto_settings and manual_settings for backward compatibility
        current_config = product.chunking_config or {}
        if not isinstance(current_config, dict):
            current_config = {}
        
        resolved_settings = {
            "content_type": best_analysis.content_type.value,
            "chunk_size": best_analysis.chunk_size,
            "chunk_overlap": best_analysis.chunk_overlap,
            "min_chunk_size": best_analysis.min_chunk_size,
            "max_chunk_size": best_analysis.max_chunk_size,
            "chunking_strategy": best_analysis.strategy.value,
            "confidence": best_analysis.confidence,
            "reasoning": best_analysis.reasoning,
        }
        
        # Preserve existing settings (backward compatibility)
        updated_config = {
            "mode": current_config.get("mode", "auto"),  # Preserve mode
            "resolved_settings": resolved_settings,  # Add/update resolved_settings
            "last_analyzed": datetime.utcnow().isoformat() + "Z",
            "analysis_confidence": best_analysis.confidence,
            "sample_files_analyzed": sample_files_analyzed,
            # Preserve existing auto_settings and manual_settings
            "auto_settings": current_config.get("auto_settings", {}),
            "manual_settings": current_config.get("manual_settings", {}),
            # Preserve other fields (e.g., preprocessing_flags, optimization_mode)
            **{k: v for k, v in current_config.items() if k not in ["resolved_settings", "last_analyzed", "analysis_confidence", "sample_files_analyzed", "mode"]}
        }
        
        updates["chunking_config"] = updated_config
        
        logger.info(f"Updated chunking_config with resolved_settings: {resolved_settings}")
    
    return updates


def task_preprocess(**context) -> Dict[str, Any]:
    """Preprocess raw data using AIRD PreprocessStage."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]
    playbook_id = params.get("playbook_id")

    logger.info(f"Starting AIRD preprocessing for product {product_id}, version {version}, playbook={playbook_id}")

    # Get AIRD context
    aird_context = get_aird_context(**context)
    storage = aird_context["storage"]
    tracker = aird_context.get("tracker")
    db = aird_context["db"]

    try:
        # Verify product exists first (enterprise best practice: validate before query)
        product = db.query(Product).filter(Product.id == product_id).first()
        if not product:
            logger.error(f"Product {product_id} not found in database")
            raise ValueError(f"Product {product_id} not found. Cannot process raw files for non-existent product.")

        logger.info(f"Found product: {product.name} (id: {product_id}, current_version: {product.current_version})")

        # Verify version matches (safety check)
        if version and product.current_version and version > product.current_version:
            logger.warning(f"Pipeline version {version} is greater than product current_version {product.current_version}")
            logger.info("This is normal for new ingestions - raw files may exist for this version")

        # Get playbook_id from product if not provided
        if product.playbook_id and not playbook_id:
            playbook_id = product.playbook_id
            logger.info(f"Using playbook from product: {playbook_id}")

        # Enterprise best practice: Query raw files with explicit type conversion and logging
        logger.info(
            f"Querying raw files for product_id={product_id} (type: {type(product_id).__name__}), version={version} (type: {type(version).__name__})"
        )

        # Ensure product_id is UUID type for query
        query_product_id = UUID(str(product_id)) if not isinstance(product_id, UUID) else product_id
        query_version = int(version) if version is not None else None

        if query_version is None:
            logger.error("Version is None - cannot query raw files without version")
            raise ValueError("Version parameter is required to query raw files")

        # Query raw files from database (stored during initial ingest)
        # Filter by exact product_id and version match (enterprise best practice: strict filtering)
        raw_file_records = (
            db.query(RawFile)
            .filter(
                RawFile.product_id == query_product_id,
                RawFile.version == query_version,
                RawFile.status != RawFileStatus.DELETED,  # Exclude deleted files
            )
            .all()
        )

        logger.info(f"Found {len(raw_file_records)} raw file records in database for product {product_id}, version {version}")

        # Additional verification: Check if any raw files exist for this product at all (helpful for debugging)
        all_product_files = db.query(RawFile).filter(RawFile.product_id == query_product_id).all()
        if all_product_files and not raw_file_records:
            # Files exist for product but not for this version
            versions_available = set(f.version for f in all_product_files)
            logger.warning(f"Raw files exist for product {product_id} but not for version {version}")
            logger.info(f"Available versions for this product: {sorted(versions_available)}")
            logger.info("Hint: Make sure initial ingest was run for the correct version")
            logger.info(
                f"Pipeline is looking for version {version}, but files exist for versions: {sorted(versions_available)}"
            )
        elif raw_file_records:
            # Log the actual files found for verification
            file_info = [
                f"{rf.filename} (stem: {rf.file_stem}, status: {rf.status.value}, storage_key: {rf.storage_key})"
                for rf in raw_file_records[:10]
            ]  # Log first 10 files
            logger.info(f"Raw files found for product {product_id}, version {version}:")
            for info in file_info:
                logger.info(f"  - {info}")
            if len(raw_file_records) > 10:
                logger.info(f"  ... and {len(raw_file_records) - 10} more files")

            # Verify all files belong to the correct product and version
            mismatched = [rf for rf in raw_file_records if rf.product_id != query_product_id or rf.version != query_version]
            if mismatched:
                logger.error(f"CRITICAL: Found {len(mismatched)} raw files with mismatched product_id or version!")
                for rf in mismatched:
                    logger.error(
                        f"  Mismatch: file_id={rf.id}, expected_product={query_product_id}, got_product={rf.product_id}, "
                        f"expected_version={query_version}, got_version={rf.version}"
                    )
                raise ValueError(
                    f"Database integrity error: {len(mismatched)} raw files have mismatched product_id or version"
                )
            else:
                logger.info(f"‚úì All {len(raw_file_records)} raw files verified: correct product_id and version")

        if not raw_file_records:
            error_msg = (
                f"No raw files found in database for product {product_id}, version {version}. Please run initial ingest first."
            )
            logger.error(f"‚ùå PREPROCESSING FAILED: {error_msg}")
            logger.info("Raw files should be ingested first using 'Run initial ingest' button")

            # Raise exception to mark Airflow task as FAILED (this is a failure condition, not a skip)
            raise RuntimeError(error_msg)

        # Enterprise best practice: Validate files exist in MinIO before processing
        validated_files = []
        files_missing = []

        for record in raw_file_records:
            # Check if file exists in MinIO
            try:
                exists = minio_client.object_exists(record.storage_bucket, record.storage_key)
                if exists:
                    validated_files.append(record.file_stem)
                    # Update status to processing
                    if record.status == RawFileStatus.INGESTED:
                        record.status = RawFileStatus.PROCESSING
                else:
                    files_missing.append(record.file_stem)
                    logger.warning(f"File missing in MinIO but exists in DB: {record.storage_key}")
                    record.status = RawFileStatus.FAILED
                    record.error_message = f"File not found in MinIO: {record.storage_key}"
            except Exception as e:
                logger.error(f"Error checking file existence for {record.storage_key}: {e}")
                files_missing.append(record.file_stem)
                record.status = RawFileStatus.FAILED
                record.error_message = f"Error validating file: {str(e)}"

        # Commit status updates
        if files_missing:
            db.commit()
            logger.warning(f"{len(files_missing)} files missing in MinIO: {files_missing}")

        if not validated_files:
            error_msg = f"No valid files found. {len(files_missing)} files missing in MinIO."
            logger.error(f"‚ùå PREPROCESSING FAILED: {error_msg}")
            logger.error(f"Missing files: {files_missing}")

            # Mark all raw files as failed
            for record in raw_file_records:
                if record.status != RawFileStatus.FAILED:
                    record.status = RawFileStatus.FAILED
                    record.error_message = error_msg
            db.commit()

            # Raise exception to mark Airflow task as FAILED
            raise RuntimeError(error_msg)

        # Extract file stems and create mapping to storage_keys for accurate file retrieval
        raw_files = validated_files

        # Create mapping: file_stem -> storage_key for accurate file retrieval
        # This ensures we use the exact storage_key from database, not construct a path with .txt extension
        file_stem_to_storage_key = {}
        for record in raw_file_records:
            if record.file_stem in raw_files:
                file_stem_to_storage_key[record.file_stem] = {
                    "storage_key": record.storage_key,
                    "storage_bucket": record.storage_bucket,
                    "filename": record.filename,
                }

        logger.info(
            f"Found {len(raw_files)} validated raw files in database to process (out of {len(raw_file_records)} total)"
        )
        logger.info(f"Raw files will be read from MinIO using stored keys")
        logger.info(f"File stems to process: {raw_files}")
        logger.info(f"file_stem_to_storage_key mapping: {file_stem_to_storage_key}")

        # Auto-detect playbook and chunking configuration if needed
        # This runs BEFORE preprocessing stage to set up configuration
        # Refresh product to get latest state
        db.refresh(product)
        needs_auto_detection = (
            (product.playbook_id is None)  # Auto-detect playbook
            or (
                product.chunking_config
                and isinstance(product.chunking_config, dict)
                and product.chunking_config.get("mode") == "auto"  # Auto-detect chunking
            )
        )
        
        if needs_auto_detection:
            # Get validated raw file records (only files that exist in MinIO)
            validated_raw_file_records = [rf for rf in raw_file_records if rf.file_stem in validated_files]
            
            if validated_raw_file_records:
                logger.info(f"üîç Starting auto-detection for playbook and/or chunking configuration...")
                logger.info(f"  - Playbook auto-detect: {product.playbook_id is None}")
                logger.info(f"  - Chunking auto-detect: {product.chunking_config and isinstance(product.chunking_config, dict) and product.chunking_config.get('mode') == 'auto'}")
                logger.info(f"  - Sampling from {len(validated_raw_file_records)} validated files")
                
                try:
                    auto_detection_updates = auto_detect_playbook_and_chunking(
                        product=product,
                        raw_file_records=validated_raw_file_records,
                        storage=storage,
                        db=db,
                    )
                    
                    if auto_detection_updates:
                        logger.info(f"‚úÖ Auto-detection completed: {list(auto_detection_updates.keys())}")
                        
                        # Update product with detected values
                        if "playbook_id" in auto_detection_updates:
                            product.playbook_id = auto_detection_updates["playbook_id"]
                            playbook_id = auto_detection_updates["playbook_id"]  # Update local variable
                            logger.info(f"  ‚Üí Updated playbook_id: {product.playbook_id}")
                        
                        if "playbook_selection" in auto_detection_updates:
                            product.playbook_selection = auto_detection_updates["playbook_selection"]
                            logger.info(f"  ‚Üí Updated playbook_selection: {auto_detection_updates['playbook_selection']}")
                        
                        if "chunking_config" in auto_detection_updates:
                            from sqlalchemy.orm.attributes import flag_modified
                            product.chunking_config = auto_detection_updates["chunking_config"]
                            flag_modified(product, "chunking_config")
                            chunking_config = auto_detection_updates["chunking_config"]  # Update local variable
                            logger.info(f"  ‚Üí Updated chunking_config with resolved_settings")
                        
                        # Commit updates to database
                        db.commit()
                        db.refresh(product)
                        logger.info("‚úÖ Auto-detection updates committed to database")
                    else:
                        logger.info("‚ÑπÔ∏è No auto-detection updates needed or available")
                except Exception as e:
                    logger.error(f"‚ùå Auto-detection failed: {e}", exc_info=True)
                    std_logger.error(f"Auto-detection failed: {e}", exc_info=True)
                    # Continue with preprocessing even if auto-detection fails
                    # The existing playbook/chunking config will be used
                    logger.warning("Continuing with preprocessing using existing/default configuration")
            else:
                logger.warning("‚ö†Ô∏è Auto-detection skipped: no validated raw files available")
        else:
            logger.info("‚ÑπÔ∏è Auto-detection not needed: playbook_id is set and chunking mode is not auto")

        # Get playbook_id from product if updated (or from params)
        if product.playbook_id and not playbook_id:
            playbook_id = product.playbook_id
            logger.info(f"Using playbook from product (after auto-detection): {playbook_id}")

        # Create and execute preprocessing stage
        # Lazy import to avoid DAG import timeouts
        from primedata.ingestion_pipeline.aird_stages.preprocess import PreprocessStage

        logger.info(
            f"Creating PreprocessStage instance: product_id={product_id}, version={version}, workspace_id={workspace_id}, playbook_id={playbook_id}"
        )
        try:
            preprocess_stage = PreprocessStage(
                product_id=product_id,
                version=version,
                workspace_id=workspace_id,
                config={"playbook_id": playbook_id} if playbook_id else {},
            )
            logger.info(f"PreprocessStage instance created successfully")
        except Exception as e:
            logger.error(f"Failed to create PreprocessStage instance: {type(e).__name__}: {str(e)}", exc_info=True)
            import traceback

            logger.error(f"PreprocessStage creation traceback:\n{traceback.format_exc()}")
            raise

        # Get chunking config from product or params
        chunking_config = params.get("chunking_config")
        if not chunking_config and product:
            # Refresh product to get latest chunking_config (including resolved_settings from auto-detection)
            db.refresh(product)
            chunking_config = product.chunking_config

        # Log preprocessing flags if present (from recommendations)
        if chunking_config and isinstance(chunking_config, dict):
            preprocessing_flags = chunking_config.get("preprocessing_flags", {})
            if preprocessing_flags:
                logger.info(f"‚úÖ Preprocessing flags detected (from recommendations): {preprocessing_flags}")
                if preprocessing_flags.get("enhanced_normalization"):
                    logger.info("  ‚Üí Enhanced normalization will be applied")
                if preprocessing_flags.get("error_correction"):
                    logger.info("  ‚Üí Error correction will be applied")
                if preprocessing_flags.get("force_metadata_extraction") or preprocessing_flags.get(
                    "additional_metadata_fields"
                ):
                    logger.info("  ‚Üí Enhanced metadata extraction will be applied")

        stage_context = {
            "storage": storage,
            "raw_files": raw_files,
            "playbook_id": playbook_id,
            "file_stem_to_storage_key": file_stem_to_storage_key,  # Pass mapping for accurate file retrieval
            "chunking_config": chunking_config,  # Pass product chunking config (including preprocessing_flags)
            "workspace_id": workspace_id,  # For loading custom playbooks
            "db": db,  # For loading custom playbooks
        }
        logger.info(f"Executing PreprocessStage with context keys: {list(stage_context.keys())}")
        logger.info(f"Context storage type: {type(stage_context['storage']).__name__}")
        logger.info(f"Context raw_files: {stage_context['raw_files']}")
        logger.info(f"Context file_stem_to_storage_key keys: {list(stage_context['file_stem_to_storage_key'].keys())}")

        try:
            result = preprocess_stage.execute(stage_context)
            logger.info(f"PreprocessStage.execute() completed: status={result.status.value}")
        except Exception as e:
            logger.error(f"EXCEPTION during PreprocessStage.execute(): {type(e).__name__}: {str(e)}", exc_info=True)
            import traceback

            logger.error(f"PreprocessStage.execute() traceback:\n{traceback.format_exc()}")
            raise

        # Track stage result
        if tracker:
            tracker.record_stage_result(result)

            # Update product preprocessing stats (save to S3 if large)
            if product and result.status == StageStatus.SUCCEEDED:
                from primedata.services.s3_json_storage import save_product_json_field

                s3_path, should_save_to_s3 = save_product_json_field(
                    product.workspace_id, product.id, "preprocessing_stats", result.metrics
                )
                if should_save_to_s3 and s3_path:
                    product.preprocessing_stats_path = s3_path
                    product.preprocessing_stats = None  # Clear DB field
                else:
                    product.preprocessing_stats = result.metrics
                    product.preprocessing_stats_path = None  # Clear S3 path if exists
                # Get playbook_id from result metrics (if auto-routed) or use the one from params
                final_playbook_id = result.metrics.get("playbook_id") or playbook_id
                if final_playbook_id:
                    product.playbook_id = final_playbook_id
                    logger.info(f"Updated product {product_id} with playbook_id: {final_playbook_id}")

                # Persist resolved chunking configuration if available (so UI can display actual used config)
                # Preserve original auto_settings and manual_settings while adding resolved_settings
                resolved_chunking = result.metrics.get("chunking_config_used")
                if resolved_chunking:
                    # Preserve existing config structure
                    current_config = product.chunking_config or {}
                    current_config.update(
                        {
                            "mode": resolved_chunking.get("mode", current_config.get("mode", "auto")),
                            "resolved_settings": resolved_chunking,
                            # Preserve auto_settings and manual_settings if they exist
                            "auto_settings": current_config.get("auto_settings", {}),
                            "manual_settings": current_config.get("manual_settings", {}),
                        }
                    )
                    product.chunking_config = current_config
                    logger.info(f"Updated product {product_id} with chunking_config: {product.chunking_config}")

                    # Store chunking config resolved_settings in pipeline_run metrics for UI display
                    if aird_context.get("pipeline_run") and resolved_chunking:
                        pipeline_run = aird_context["pipeline_run"]
                        if pipeline_run.metrics is None:
                            pipeline_run.metrics = {}
                        
                        # Store resolved_settings for this pipeline run
                        pipeline_run.metrics["chunking_config"] = {
                            "resolved_settings": resolved_chunking,
                            "timestamp": datetime.utcnow().isoformat(),
                            "version": version
                        }
                        # Use flag_modified to ensure SQLAlchemy detects the change
                        from sqlalchemy.orm.attributes import flag_modified
                        flag_modified(pipeline_run, "metrics")
                        db.commit()
                        logger.info(f"Stored chunking_config resolved_settings in pipeline_run {pipeline_run.id} metrics")

                # Store playbook selection metadata for verification
                playbook_selection = result.metrics.get("playbook_selection")
                if playbook_selection:
                    # Ensure playbook_id is included in metadata
                    playbook_selection["playbook_id"] = final_playbook_id
                    product.playbook_selection = playbook_selection
                    logger.info(f"Updated product {product_id} with playbook_selection: {playbook_selection}")
                elif final_playbook_id and not product.playbook_selection:
                    # If playbook was provided but no metadata exists, mark as manual
                    product.playbook_selection = {
                        "playbook_id": final_playbook_id,
                        "method": "manual",
                        "reason": None,
                        "detected_at": None,
                    }

            # IMPORTANT: Do NOT mark raw files as PROCESSED here - only mark them in finalize task
            # when entire pipeline completes successfully. Files remain PROCESSING during pipeline run.
            # This allows pipeline to be retried if later stages fail.
            db.commit()

        logger.info(f"Preprocessing completed: {result.status.value}, chunks={result.metrics.get('total_chunks', 0)}")

        # Phase 1 & 2: Register artifacts for traceability
        preprocess_artifact_ids = []
        if result.status == StageStatus.SUCCEEDED and aird_context.get("pipeline_run"):
            try:
                preprocess_artifact_ids = register_stage_artifacts(
                    db=db,
                    pipeline_run_id=aird_context["pipeline_run"].id,
                    workspace_id=workspace_id,
                    product_id=product_id,
                    version=version,
                    stage_name="preprocess",
                    result=result,
                    storage=storage,
                    input_artifact_ids=None,  # Preprocess doesn't have input artifacts (raw files are tracked separately)
                )
                logger.info(f"Registered {len(preprocess_artifact_ids)} preprocessing artifacts")
                std_logger.info(f"Registered {len(preprocess_artifact_ids)} preprocessing artifacts")
            except Exception as e:
                logger.error(f"Failed to register preprocessing artifacts: {e}", exc_info=True)
                std_logger.error(f"Failed to register preprocessing artifacts: {e}", exc_info=True)
                # Don't fail the task if artifact registration fails

        # Store result in XCom for next stages (store before raising exception)
        context["task_instance"].xcom_push(
            key="preprocess_result",
            value={
                "status": result.status.value,
                "metrics": result.metrics,
                "processed_file_list": result.metrics.get("processed_file_list", []),
                "artifact_ids": [str(aid) for aid in preprocess_artifact_ids],  # Phase 2: Pass artifact IDs for lineage
            },
        )

        # Mark raw files as failed if preprocessing failed (before raising exception)
        if result.status == StageStatus.FAILED:
            error_msg = result.error or "Unknown error"
            failed_files = result.metrics.get("failed_file_list", [])

            # Mark raw files as failed
            for record in raw_file_records:
                if record.status == RawFileStatus.PROCESSING:
                    record.status = RawFileStatus.FAILED
                    record.error_message = error_msg
            db.commit()

            # Include failed files in error context
            context_msg = f"Failed files: {failed_files}" if failed_files else ""
        else:
            context_msg = ""

        # Raise exception if stage failed (Airflow only fails tasks on exceptions)
        # This will mark the Airflow task as FAILED and stop downstream tasks
        raise_if_stage_failed(result, "Preprocessing", context_msg)

        # Return success metrics only if preprocessing succeeded
        return {
            "status": result.status.value,
            "files_count": result.metrics.get("processed_files", 0),
            "total_chunks": result.metrics.get("total_chunks", 0),
            "playbook_id": result.metrics.get("playbook_id"),
            "processed_file_list": result.metrics.get("processed_file_list", []),
        }

    except Exception as e:
        logger.error(f"Preprocessing failed: {e}", exc_info=True)
        std_logger.error(f"Preprocessing failed: {e}", exc_info=True)
        # Mark all currently PROCESSING raw files as FAILED
        if "raw_file_records" in locals() and "db" in locals():
            try:
                for record in raw_file_records:
                    if record.status == RawFileStatus.PROCESSING:
                        record.status = RawFileStatus.FAILED
                        record.error_message = record.error_message or f"Preprocessing failed: {str(e)}"
                db.commit()
            except Exception as db_error:
                logger.error(f"Failed to update raw file status during preprocessing error: {db_error}")
                std_logger.error(f"Failed to update raw file status during preprocessing error: {db_error}")
                db.rollback()
        raise
    finally:
        if "db" in locals():
            db.close()


def task_scoring(**context) -> Dict[str, Any]:
    """Score processed chunks using AIRD ScoringStage."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]

    logger.info(f"Starting AIRD scoring for product {product_id}, version {version}")

    aird_context = get_aird_context(**context)
    storage = aird_context["storage"]
    tracker = aird_context.get("tracker")
    db = aird_context["db"]

    try:
        # Get processed files from preprocessing
        preprocess_result = context["task_instance"].xcom_pull(task_ids="preprocess", key="preprocess_result")
        if not preprocess_result:
            preprocess_result = context["task_instance"].xcom_pull(task_ids="preprocess")
        processed_files = preprocess_result.get("processed_file_list", []) if preprocess_result else []

        if not processed_files:
            logger.warning("No processed files found from preprocessing stage")
            return {
                "status": "skipped",
                "message": "No processed files to score",
            }

        # Load playbook for AI-Ready metrics (noise patterns, coherence settings)
        playbook_id = params.get("playbook_id")
        if not playbook_id:
            # Try to get from product
            from primedata.db.models import Product
            product = db.query(Product).filter(Product.id == product_id).first()
            if product and product.playbook_id:
                playbook_id = product.playbook_id
        
        playbook = {}
        if playbook_id:
            try:
                from primedata.ingestion_pipeline.aird_stages.playbooks import load_playbook_yaml
                playbook = load_playbook_yaml(playbook_id, workspace_id=str(workspace_id), db_session=db)
                logger.info(f"Loaded playbook {playbook_id} for scoring stage (keys: {list(playbook.keys())[:10]})")
                # Log if AI-Ready sections are present
                if "noise_patterns" in playbook:
                    logger.info(f"Playbook {playbook_id} has noise_patterns section")
                if "coherence" in playbook:
                    logger.info(f"Playbook {playbook_id} has coherence section")
            except Exception as e:
                logger.warning(f"Failed to load playbook {playbook_id}: {e}, using empty playbook", exc_info=True)

        # Create and execute scoring stage
        # Lazy import to avoid DAG import timeouts
        from primedata.ingestion_pipeline.aird_stages.scoring import ScoringStage

        scoring_stage = ScoringStage(
            product_id=product_id,
            version=version,
            workspace_id=workspace_id,
        )

        stage_context = {
            "storage": storage,
            "processed_files": processed_files,
            "preprocess_result": preprocess_result,
            "playbook": playbook,
            "playbook_id": playbook_id,
        }

        result = scoring_stage.execute(stage_context)

        if tracker:
            tracker.record_stage_result(result)

        logger.info(f"Scoring completed: {result.status.value}, chunks={result.metrics.get('total_chunks', 0)}")

        # Phase 1 & 2: Register artifacts for traceability
        scoring_artifact_ids = []
        if result.status == StageStatus.SUCCEEDED and aird_context.get("pipeline_run"):
            try:
                # Get input artifact IDs from preprocess stage (for lineage)
                preprocess_artifact_ids_str = preprocess_result.get("artifact_ids", []) if preprocess_result else []
                input_artifact_ids = (
                    [UUID(aid) for aid in preprocess_artifact_ids_str] if preprocess_artifact_ids_str else None
                )

                scoring_artifact_ids = register_stage_artifacts(
                    db=db,
                    pipeline_run_id=aird_context["pipeline_run"].id,
                    workspace_id=workspace_id,
                    product_id=product_id,
                    version=version,
                    stage_name="scoring",
                    result=result,
                    storage=storage,
                    input_artifact_ids=input_artifact_ids,
                )
                logger.info(f"Registered {len(scoring_artifact_ids)} scoring artifacts")
                std_logger.info(f"Registered {len(scoring_artifact_ids)} scoring artifacts")
            except Exception as e:
                logger.error(f"Failed to register scoring artifacts: {e}", exc_info=True)
                std_logger.error(f"Failed to register scoring artifacts: {e}", exc_info=True)

        # Store result in XCom (store before raising exception if failed)
        context["task_instance"].xcom_push(
            key="scoring_result",
            value={
                "status": result.status.value,
                "metrics": result.metrics,
                "artifact_ids": [str(aid) for aid in scoring_artifact_ids],  # Phase 2: Pass artifact IDs for lineage
            },
        )

        # Mark raw files as failed if scoring failed (before raising exception)
        if result.status == StageStatus.FAILED:
            error_msg = result.error or "Unknown error"
            mark_raw_files_as_failed(product_id, version, f"Scoring stage failed: {error_msg}", db_session=db)

        # Raise exception if stage failed (Airflow only fails tasks on exceptions)
        raise_if_stage_failed(result, "Scoring")

        return {
            "status": result.status.value,
            "total_chunks": result.metrics.get("total_chunks", 0),
            "avg_trust_score": result.metrics.get("avg_trust_score", 0.0),
        }
    except Exception as e:
        logger.error(f"Scoring failed: {e}", exc_info=True)
        std_logger.error(f"Scoring failed: {e}", exc_info=True)
        # Mark raw files as FAILED since pipeline failed at scoring stage
        mark_raw_files_as_failed(product_id, version, f"Scoring stage failed: {str(e)}", db_session=db)
        raise
    finally:
        db.close()


def task_fingerprint(**context) -> Dict[str, Any]:
    """Generate readiness fingerprint using AIRD FingerprintStage."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]

    logger.info(f"Starting AIRD fingerprint generation for product {product_id}, version {version}")

    aird_context = get_aird_context(**context)
    storage = aird_context["storage"]
    tracker = aird_context.get("tracker")
    db = aird_context["db"]

    try:
        # Get scoring result
        scoring_result = context["task_instance"].xcom_pull(task_ids="scoring", key="scoring_result")
        if not scoring_result:
            scoring_result = context["task_instance"].xcom_pull(task_ids="scoring")

        # Get preprocess result for preprocessing stats (needed for Chunk Boundary Quality)
        preprocess_result = context["task_instance"].xcom_pull(task_ids="preprocess", key="preprocess_result")
        if not preprocess_result:
            preprocess_result = context["task_instance"].xcom_pull(task_ids="preprocess")

        # Create and execute fingerprint stage
        # Lazy import to avoid DAG import timeouts
        from primedata.ingestion_pipeline.aird_stages.fingerprint import FingerprintStage

        fingerprint_stage = FingerprintStage(
            product_id=product_id,
            version=version,
            workspace_id=workspace_id,
        )

        stage_context = {
            "storage": storage,
            "scoring_result": scoring_result,
            "preprocess_result": preprocess_result,
        }

        result = fingerprint_stage.execute(stage_context)

        if tracker:
            tracker.record_stage_result(result)

        logger.info(f"Fingerprint generation completed: {result.status.value}")

        # Phase 1 & 2: Register artifacts for traceability
        fingerprint_artifact_ids = []
        if result.status == StageStatus.SUCCEEDED and aird_context.get("pipeline_run"):
            try:
                # Get input artifact IDs from scoring stage (for lineage)
                scoring_artifact_ids_str = scoring_result.get("artifact_ids", []) if scoring_result else []
                input_artifact_ids = [UUID(aid) for aid in scoring_artifact_ids_str] if scoring_artifact_ids_str else None

                fingerprint_artifact_ids = register_stage_artifacts(
                    db=db,
                    pipeline_run_id=aird_context["pipeline_run"].id,
                    workspace_id=workspace_id,
                    product_id=product_id,
                    version=version,
                    stage_name="fingerprint",
                    result=result,
                    storage=storage,
                    input_artifact_ids=input_artifact_ids,
                )
                logger.info(f"Registered {len(fingerprint_artifact_ids)} fingerprint artifacts")
                std_logger.info(f"Registered {len(fingerprint_artifact_ids)} fingerprint artifacts")
            except Exception as e:
                logger.error(f"Failed to register fingerprint artifacts: {e}", exc_info=True)
                std_logger.error(f"Failed to register fingerprint artifacts: {e}", exc_info=True)

        # Store result in XCom (store before raising exception if failed)
        context["task_instance"].xcom_push(
            key="fingerprint_result",
            value={
                "status": result.status.value,
                "metrics": result.metrics,
                "artifact_ids": [str(aid) for aid in fingerprint_artifact_ids],  # Phase 2: Pass artifact IDs for lineage
            },
        )

        # Mark raw files as failed if fingerprint failed (before raising exception)
        if result.status == StageStatus.FAILED:
            error_msg = result.error or "Unknown error"
            mark_raw_files_as_failed(product_id, version, f"Fingerprint stage failed: {error_msg}", db_session=db)

        # Raise exception if stage failed (Airflow only fails tasks on exceptions)
        raise_if_stage_failed(result, "Fingerprint")

        # Update product readiness fingerprint and trust score (save to S3 if large)
        product = db.query(Product).filter(Product.id == product_id).first()
        if product and result.status == StageStatus.SUCCEEDED:
            # Extract fingerprint from result.metrics (fingerprint is nested inside metrics)
            fingerprint = result.metrics.get("fingerprint", {})
            if fingerprint:
                from primedata.services.s3_json_storage import save_product_json_field

                s3_path, should_save_to_s3 = save_product_json_field(
                    product.workspace_id, product.id, "readiness_fingerprint", fingerprint
                )
                if should_save_to_s3 and s3_path:
                    product.readiness_fingerprint_path = s3_path
                    product.readiness_fingerprint = None  # Clear DB field
                else:
                    product.readiness_fingerprint = fingerprint
                    product.readiness_fingerprint_path = None  # Clear S3 path if exists
                # Extract AI_Trust_Score from fingerprint and set trust_score
                trust_score = fingerprint.get("AI_Trust_Score")
                if trust_score is not None:
                    product.trust_score = float(trust_score)  # Ensure it's a float
                else:
                    # Fallback: try to get trust_score from metrics_result directly
                    trust_score = result.metrics.get("trust_score")
                    if trust_score is not None:
                        product.trust_score = float(trust_score)
                db.commit()
                logger.info(f"Updated product {product_id} with fingerprint and trust_score={product.trust_score}")
            else:
                logger.warning(f"No fingerprint found in result.metrics for product {product_id}")

        return {
            "status": result.status.value,
            "metrics": result.metrics,
        }
    except Exception as e:
        logger.error(f"Fingerprint generation failed: {e}", exc_info=True)
        std_logger.error(f"Fingerprint generation failed: {e}", exc_info=True)
        # Mark raw files as FAILED since pipeline failed at fingerprint stage
        mark_raw_files_as_failed(product_id, version, f"Fingerprint stage failed: {str(e)}", db_session=db)
        raise
    finally:
        db.close()


def task_validation(**context) -> Dict[str, Any]:
    """Generate validation summary using AIRD ValidationStage."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]

    logger.info(f"Starting AIRD validation summary generation for product {product_id}, version {version}")

    aird_context = get_aird_context(**context)
    storage = aird_context["storage"]
    tracker = aird_context.get("tracker")
    db = aird_context["db"]

    try:
        # Get scoring result
        scoring_result = context["task_instance"].xcom_pull(task_ids="scoring", key="scoring_result")
        if not scoring_result:
            scoring_result = context["task_instance"].xcom_pull(task_ids="scoring")

        # Create and execute validation stage
        # Lazy import to avoid DAG import timeouts
        from primedata.ingestion_pipeline.aird_stages.validation import ValidationStage

        validation_stage = ValidationStage(
            product_id=product_id,
            version=version,
            workspace_id=workspace_id,
        )

        stage_context = {
            "storage": storage,
            "scoring_result": scoring_result,
        }

        result = validation_stage.execute(stage_context)

        if tracker:
            tracker.record_stage_result(result)

        logger.info(f"Validation summary generation completed: {result.status.value}")

        # Phase 1 & 2: Register artifacts for traceability
        validation_artifact_ids = []
        if result.status == StageStatus.SUCCEEDED and aird_context.get("pipeline_run"):
            try:
                # Get input artifact IDs from scoring stage (for lineage)
                scoring_result = context["task_instance"].xcom_pull(task_ids="scoring", key="scoring_result")
                if not scoring_result:
                    scoring_result = context["task_instance"].xcom_pull(task_ids="scoring")
                scoring_artifact_ids_str = scoring_result.get("artifact_ids", []) if scoring_result else []
                input_artifact_ids = [UUID(aid) for aid in scoring_artifact_ids_str] if scoring_artifact_ids_str else None

                validation_artifact_ids = register_stage_artifacts(
                    db=db,
                    pipeline_run_id=aird_context["pipeline_run"].id,
                    workspace_id=workspace_id,
                    product_id=product_id,
                    version=version,
                    stage_name="validation",
                    result=result,
                    storage=storage,
                    input_artifact_ids=input_artifact_ids,
                )
                logger.info(f"Registered {len(validation_artifact_ids)} validation artifacts")
                std_logger.info(f"Registered {len(validation_artifact_ids)} validation artifacts")
            except Exception as e:
                logger.error(f"Failed to register validation artifacts: {e}", exc_info=True)
                std_logger.error(f"Failed to register validation artifacts: {e}", exc_info=True)

        # Raise exception if stage failed (Airflow only fails tasks on exceptions)
        raise_if_stage_failed(result, "Validation")

        # Update product validation summary path
        product = db.query(Product).filter(Product.id == product_id).first()
        if product and result.status == StageStatus.SUCCEEDED:
            if "validation_summary_path" in result.metrics:
                product.validation_summary_path = result.metrics["validation_summary_path"]
                db.commit()

        return {
            "status": result.status.value,
            "validation_summary_path": result.metrics.get("validation_summary_path"),
        }
    except Exception as e:
        logger.error(f"Validation summary generation failed: {e}", exc_info=True)
        std_logger.error(f"Validation summary generation failed: {e}", exc_info=True)
        # Mark raw files as FAILED since pipeline failed at validation stage
        mark_raw_files_as_failed(product_id, version, f"Validation stage failed: {str(e)}", db_session=db)
        raise
    finally:
        db.close()


def task_policy(**context) -> Dict[str, Any]:
    """Evaluate policy using AIRD PolicyStage."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]

    logger.info(f"Starting AIRD policy evaluation for product {product_id}, version {version}")

    aird_context = get_aird_context(**context)
    storage = aird_context["storage"]
    tracker = aird_context.get("tracker")
    db = aird_context["db"]

    try:
        # Get fingerprint result
        fingerprint_result = context["task_instance"].xcom_pull(task_ids="fingerprint", key="fingerprint_result")
        if not fingerprint_result:
            fingerprint_result = context["task_instance"].xcom_pull(task_ids="fingerprint")

        # Create and execute policy stage
        # Lazy import to avoid DAG import timeouts
        from primedata.ingestion_pipeline.aird_stages.policy import PolicyStage

        policy_stage = PolicyStage(
            product_id=product_id,
            version=version,
            workspace_id=workspace_id,
        )

        stage_context = {
            "storage": storage,
            "fingerprint_result": fingerprint_result,
        }

        result = policy_stage.execute(stage_context)

        if tracker:
            tracker.record_stage_result(result)

        logger.info(f"Policy evaluation completed: {result.status.value}")

        # Phase 1 & 2: Register artifacts for traceability
        # Note: Policy stage doesn't generate files, but we track the evaluation result as metadata
        policy_artifact_ids = []
        if result.status == StageStatus.SUCCEEDED and aird_context.get("pipeline_run"):
            try:
                # Get input artifact IDs from fingerprint stage (for lineage)
                fingerprint_artifact_ids_str = fingerprint_result.get("artifact_ids", []) if fingerprint_result else []
                input_artifact_ids = (
                    [UUID(aid) for aid in fingerprint_artifact_ids_str] if fingerprint_artifact_ids_str else None
                )

                # Policy doesn't generate a file, but we create a virtual artifact record for traceability
                # Store policy result in metadata only (no MinIO file)
                from primedata.db.models import PipelineArtifact as PA

                # Prepare artifact metadata
                artifact_metadata_dict = {
                    "policy_passed": result.metrics.get("policy_passed", False),
                    "violations": result.metrics.get("violations", []),
                    "violations_count": result.metrics.get("violations_count", 0),
                    "thresholds": result.metrics.get("thresholds", {}),
                }

                # Calculate checksum from metadata JSON (since there's no file)
                metadata_json = json.dumps(artifact_metadata_dict, sort_keys=True)
                metadata_bytes = metadata_json.encode("utf-8")
                metadata_checksum = calculate_checksum(metadata_bytes, algorithm="sha256")

                policy_artifact = PA(
                    pipeline_run_id=aird_context["pipeline_run"].id,
                    workspace_id=workspace_id,
                    product_id=product_id,
                    version=version,
                    stage_name="policy",
                    artifact_type=ArtifactType.JSON,  # Policy result is JSON-like metadata
                    artifact_name="policy_evaluation",
                    storage_bucket="none",  # No MinIO file
                    storage_key=f"policy_result_{product_id}_v{version}",
                    file_size=0,
                    checksum=metadata_checksum,  # Calculate checksum from metadata JSON
                    storage_etag=metadata_checksum,  # Use checksum as ETag since there's no file
                    input_artifacts=(
                        [
                            {
                                "artifact_id": str(aid),
                                "stage": "fingerprint",
                                "artifact_name": "fingerprint",
                            }
                            for aid in input_artifact_ids
                        ]
                        if input_artifact_ids
                        else []
                    ),
                    artifact_metadata=artifact_metadata_dict,
                    retention_policy=RetentionPolicy.KEEP_FOREVER,  # Policy results are important
                    status=ArtifactStatus.ACTIVE,
                )
                db.add(policy_artifact)
                db.commit()
                db.refresh(policy_artifact)
                policy_artifact_ids.append(policy_artifact.id)
                logger.info(f"Registered policy evaluation artifact")
                std_logger.info(f"Registered policy evaluation artifact")
            except Exception as e:
                logger.error(f"Failed to register policy artifacts: {e}", exc_info=True)
                std_logger.error(f"Failed to register policy artifacts: {e}", exc_info=True)

        # Mark raw files as failed if policy failed (before raising exception)
        if result.status == StageStatus.FAILED:
            error_msg = result.error or "Unknown error"
            mark_raw_files_as_failed(product_id, version, f"Policy stage failed: {error_msg}", db_session=db)

        # Raise exception if stage failed (Airflow only fails tasks on exceptions)
        raise_if_stage_failed(result, "Policy")

        # Update product policy status
        product = db.query(Product).filter(Product.id == product_id).first()
        if product and result.status == StageStatus.SUCCEEDED:
            # Derive policy_status from policy_passed boolean (PolicyStage doesn't return policy_status directly)
            policy_passed = result.metrics.get("policy_passed", False)
            violations = result.metrics.get("violations", [])

            # Map policy_passed boolean to PolicyStatus enum
            if policy_passed:
                product.policy_status = PolicyStatus.PASSED
            else:
                product.policy_status = PolicyStatus.FAILED

            product.policy_violations = violations
            db.commit()
            logger.info(f"Updated product policy_status to {product.policy_status.value}, violations_count={len(violations)}")
            std_logger.info(
                f"Updated product policy_status to {product.policy_status.value}, violations_count={len(violations)}"
            )

        # Determine policy_status for return value
        policy_passed = result.metrics.get("policy_passed", False)
        policy_status_str = "passed" if policy_passed else "failed"

        return {
            "status": result.status.value,
            "policy_status": policy_status_str,
            "violations": result.metrics.get("violations", []),
        }
    except Exception as e:
        logger.error(f"Policy evaluation failed: {e}", exc_info=True)
        std_logger.error(f"Policy evaluation failed: {e}", exc_info=True)
        # Mark raw files as FAILED since pipeline failed at policy stage
        mark_raw_files_as_failed(product_id, version, f"Policy stage failed: {str(e)}", db_session=db)
        raise
    finally:
        db.close()


def task_reporting(**context) -> Dict[str, Any]:
    """Generate PDF trust report using AIRD ReportingStage."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]

    logger.info(f"Starting AIRD PDF report generation for product {product_id}, version {version}")

    aird_context = get_aird_context(**context)
    storage = aird_context["storage"]
    tracker = aird_context.get("tracker")
    db = aird_context["db"]

    try:
        # Get scoring result
        scoring_result = context["task_instance"].xcom_pull(task_ids="scoring", key="scoring_result")
        if not scoring_result:
            scoring_result = context["task_instance"].xcom_pull(task_ids="scoring")

        # Create and execute reporting stage
        # Lazy import to avoid DAG import timeouts
        from primedata.ingestion_pipeline.aird_stages.reporting import ReportingStage

        reporting_stage = ReportingStage(
            product_id=product_id,
            version=version,
            workspace_id=workspace_id,
        )

        stage_context = {
            "storage": storage,
            "scoring_result": scoring_result,
        }

        result = reporting_stage.execute(stage_context)

        if tracker:
            tracker.record_stage_result(result)

        logger.info(f"PDF report generation completed: {result.status.value}")

        # Phase 1 & 2: Register artifacts for traceability
        reporting_artifact_ids = []
        if result.status == StageStatus.SUCCEEDED and aird_context.get("pipeline_run"):
            try:
                # Get input artifact IDs from scoring stage (for lineage)
                scoring_artifact_ids_str = scoring_result.get("artifact_ids", []) if scoring_result else []
                input_artifact_ids = [UUID(aid) for aid in scoring_artifact_ids_str] if scoring_artifact_ids_str else None

                reporting_artifact_ids = register_stage_artifacts(
                    db=db,
                    pipeline_run_id=aird_context["pipeline_run"].id,
                    workspace_id=workspace_id,
                    product_id=product_id,
                    version=version,
                    stage_name="reporting",
                    result=result,
                    storage=storage,
                    input_artifact_ids=input_artifact_ids,
                )
                logger.info(f"Registered {len(reporting_artifact_ids)} reporting artifacts")
                std_logger.info(f"Registered {len(reporting_artifact_ids)} reporting artifacts")
            except Exception as e:
                logger.error(f"Failed to register reporting artifacts: {e}", exc_info=True)
                std_logger.error(f"Failed to register reporting artifacts: {e}", exc_info=True)

        # Mark raw files as failed if reporting failed (before raising exception)
        if result.status == StageStatus.FAILED:
            error_msg = result.error or "Unknown error"
            mark_raw_files_as_failed(product_id, version, f"Reporting stage failed: {error_msg}", db_session=db)

        # Raise exception if stage failed (Airflow only fails tasks on exceptions)
        raise_if_stage_failed(result, "Reporting")

        # Update product trust report path
        # The path is stored in result.artifacts['trust_report_pdf'], not in metrics
        trust_report_path = None
        if result.artifacts and "trust_report_pdf" in result.artifacts:
            trust_report_path = result.artifacts["trust_report_pdf"]

        product = db.query(Product).filter(Product.id == product_id).first()
        if product and result.status == StageStatus.SUCCEEDED and trust_report_path:
            product.trust_report_path = trust_report_path
            db.commit()

        return {
            "status": result.status.value,
            "trust_report_path": trust_report_path,
        }
    except Exception as e:
        logger.error(f"PDF report generation failed: {e}", exc_info=True)
        std_logger.error(f"PDF report generation failed: {e}", exc_info=True)
        # Mark raw files as FAILED since pipeline failed at reporting stage
        mark_raw_files_as_failed(product_id, version, f"Reporting stage failed: {str(e)}", db_session=db)
        raise
    finally:
        db.close()


def task_indexing(**context) -> Dict[str, Any]:
    """Index chunks to Qdrant using AIRD IndexingStage."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]

    logger.info(f"Starting AIRD indexing for product {product_id}, version {version}")

    aird_context = get_aird_context(**context)
    storage = aird_context["storage"]
    tracker = aird_context.get("tracker")
    db = aird_context["db"]

    try:
        # Check if vector creation is enabled for this product
        from primedata.db.models import Product
        product = db.query(Product).filter(Product.id == product_id).first()
        if not product:
            raise ValueError(f"Product {product_id} not found")
        
        if not product.vector_creation_enabled:
            logger.info(f"Vector creation is disabled for product {product_id}, skipping indexing stage")
            if tracker:
                from primedata.ingestion_pipeline.aird_stages.base import StageResult, StageStatus
                skipped_result = StageResult(
                    status=StageStatus.SKIPPED,
                    stage_name="indexing",
                    product_id=product_id,
                    version=version,
                    metrics={"points_indexed": 0, "reason": "vector_creation_enabled is False"},
                    error=None,
                )
                tracker.record_stage_result(skipped_result)
            return {
                "status": "skipped",
                "vectors_indexed": 0,
                "reason": "vector_creation_enabled is False",
            }

    except Exception as e:
        logger.error(f"Failed to check vector_creation_enabled for product {product_id}: {e}", exc_info=True)
        # Continue with indexing if we can't check the flag (fallback to default behavior)

    try:
        # Get processed files from preprocessing
        preprocess_result = context["task_instance"].xcom_pull(task_ids="preprocess", key="preprocess_result")
        if not preprocess_result:
            preprocess_result = context["task_instance"].xcom_pull(task_ids="preprocess")
        processed_files = preprocess_result.get("processed_file_list", []) if preprocess_result else []

        # Get scoring result
        scoring_result = context["task_instance"].xcom_pull(task_ids="scoring", key="scoring_result")
        if not scoring_result:
            scoring_result = context["task_instance"].xcom_pull(task_ids="scoring")

        # Create and execute indexing stage
        # Lazy import to avoid DAG import timeouts
        from primedata.ingestion_pipeline.aird_stages.indexing import IndexingStage

        indexing_stage = IndexingStage(
            product_id=product_id,
            version=version,
            workspace_id=workspace_id,
        )

        stage_context = {
            "storage": storage,
            "processed_files": processed_files,
            "preprocess_result": preprocess_result,
            "scoring_result": scoring_result,
        }

        result = indexing_stage.execute(stage_context)

        if tracker:
            tracker.record_stage_result(result)

        logger.info(f"Indexing completed: {result.status.value}, vectors={result.metrics.get('points_indexed', 0)}")

        # Phase 1 & 2: Register artifacts for traceability
        indexing_artifact_ids = []
        if result.status == StageStatus.SUCCEEDED and aird_context.get("pipeline_run"):
            try:
                # Get input artifact IDs from preprocess AND scoring (for lineage)
                preprocess_artifact_ids_str = preprocess_result.get("artifact_ids", []) if preprocess_result else []
                scoring_artifact_ids_str = scoring_result.get("artifact_ids", []) if scoring_result else []

                # Combine input artifacts from both stages
                input_artifact_ids = []
                if preprocess_artifact_ids_str:
                    input_artifact_ids.extend([UUID(aid) for aid in preprocess_artifact_ids_str])
                if scoring_artifact_ids_str:
                    input_artifact_ids.extend([UUID(aid) for aid in scoring_artifact_ids_str])

                indexing_artifact_ids = register_stage_artifacts(
                    db=db,
                    pipeline_run_id=aird_context["pipeline_run"].id,
                    workspace_id=workspace_id,
                    product_id=product_id,
                    version=version,
                    stage_name="indexing",
                    result=result,
                    storage=storage,
                    input_artifact_ids=input_artifact_ids if input_artifact_ids else None,
                )
                logger.info(f"Registered {len(indexing_artifact_ids)} indexing artifacts")
                std_logger.info(f"Registered {len(indexing_artifact_ids)} indexing artifacts")
            except Exception as e:
                logger.error(f"Failed to register indexing artifacts: {e}", exc_info=True)
                std_logger.error(f"Failed to register indexing artifacts: {e}", exc_info=True)

        # Mark raw files as failed if indexing failed (before raising exception)
        if result.status == StageStatus.FAILED:
            error_msg = result.error or "Unknown error"
            mark_raw_files_as_failed(product_id, version, f"Indexing stage failed: {error_msg}", db_session=db)

        # Raise exception if stage failed (Airflow only fails tasks on exceptions)
        raise_if_stage_failed(result, "Indexing")

        return {
            "status": result.status.value,
            "vectors_indexed": result.metrics.get("points_indexed", 0),  # Indexing stage uses 'points_indexed'
        }
    except Exception as e:
        logger.error(f"Indexing failed: {e}", exc_info=True)
        std_logger.error(f"Indexing failed: {e}", exc_info=True)
        # Mark raw files as FAILED since pipeline failed at indexing stage
        mark_raw_files_as_failed(product_id, version, f"Indexing stage failed: {str(e)}", db_session=db)
        raise
    finally:
        db.close()


def task_validate_data_quality(**context) -> Dict[str, Any]:
    """Validate data quality rules and generate violations."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]
    pipeline_run_id = get_pipeline_run_id_from_context(**context)

    logger.info(f"Starting data quality validation for product {product_id}, version {version}")

    # Get database session
    db = next(get_db())

    try:
        # Import here to avoid circular imports
        from primedata.db.models import DqViolation
        from primedata.db.models_enterprise import DataQualityRule

        # Get current data quality rules for the product
        rules = (
            db.query(DataQualityRule)
            .filter(DataQualityRule.product_id == product_id, DataQualityRule.is_current == True)
            .all()
        )

        logger.info(f"Found {len(rules)} data quality rules for product {product_id}")

        if not rules:
            logger.info("No data quality rules found, skipping validation")
            return {
                "status": "skipped",
                "message": "No data quality rules configured",
            }

        violations_count = 0

        for rule in rules:
            if not rule.enabled:
                logger.info(f"Skipping disabled rule: {rule.name}")
                continue

            logger.info(f"Evaluating rule: {rule.name} ({rule.rule_type})")
            # Add rule evaluation logic here
            # For now, this is a placeholder

        logger.info(f"Data quality validation completed: {violations_count} violations found")

        return {
            "status": "completed",
            "violations_count": violations_count,
        }

    except Exception as e:
        logger.error(f"Data quality validation failed: {e}", exc_info=True)
        std_logger.error(f"Data quality validation failed: {e}", exc_info=True)
        # Mark raw files as FAILED since pipeline failed at data quality validation stage
        mark_raw_files_as_failed(product_id, version, f"Data quality validation stage failed: {str(e)}", db_session=db)
        raise
    finally:
        db.close()


def task_finalize(**context) -> Dict[str, Any]:
    """Finalize pipeline and update product status."""
    params = get_dag_params(**context)
    workspace_id = params["workspace_id"]
    product_id = params["product_id"]
    version = params["version"]
    pipeline_run_id = get_pipeline_run_id_from_context(**context)

    logger.info(f"Finalizing pipeline for product {product_id}, version {version}")
    std_logger.info(f"Finalizing pipeline for product {product_id}, version {version}")

    # Get database session
    db = next(get_db())

    try:
        # Update pipeline run status
        if pipeline_run_id:
            update_pipeline_status(
                pipeline_run_id,
                "succeeded",
                {
                    "pipeline_duration": "complete",
                    "product_status": "ready",
                    "version": version,
                },
            )

        # Update product status
        product = db.query(Product).filter(Product.id == product_id).first()
        if product:
            product.status = "ready"
            product.current_version = version
            db.commit()
            logger.info(f"Product {product_id} marked as ready with version {version}")
            std_logger.info(f"Product {product_id} marked as ready with version {version}")

        # Mark all PROCESSING raw files as PROCESSED since pipeline completed successfully
        # This is the ONLY place where files should be marked as PROCESSED
        raw_files_to_finalize = (
            db.query(RawFile)
            .filter(RawFile.product_id == product_id, RawFile.version == version, RawFile.status == RawFileStatus.PROCESSING)
            .all()
        )

        if raw_files_to_finalize:
            for record in raw_files_to_finalize:
                record.status = RawFileStatus.PROCESSED
                record.processed_at = datetime.utcnow()
                record.error_message = None  # Clear any previous error messages
            db.commit()
            logger.info(
                f"Marked {len(raw_files_to_finalize)} raw files as PROCESSED for product {product_id}, version {version}"
            )
            std_logger.info(
                f"Marked {len(raw_files_to_finalize)} raw files as PROCESSED for product {product_id}, version {version}"
            )
        else:
            logger.warning(f"No PROCESSING raw files found to finalize for product {product_id}, version {version}")
            std_logger.warning(f"No PROCESSING raw files found to finalize for product {product_id}, version {version}")

        # Phase 1: Add artifact summary to pipeline_runs.metrics (lightweight metadata)
        if pipeline_run_id:
            try:
                # Convert pipeline_run_id to UUID if it's a string
                pipeline_run_uuid = UUID(pipeline_run_id) if isinstance(pipeline_run_id, str) else pipeline_run_id
                pipeline_run = db.query(PipelineRun).filter(PipelineRun.id == pipeline_run_uuid).first()

                if pipeline_run:
                    # Refresh the session to ensure we see all committed artifacts
                    db.expire_all()

                    artifact_summary = get_artifact_summary_for_run(db, pipeline_run_uuid)

                    # If no artifacts found by pipeline_run_id, try querying by product_id and version as fallback
                    if artifact_summary["total_artifacts"] == 0:
                        logger.warning(
                            f"No artifacts found for pipeline_run_id={pipeline_run_uuid}, trying fallback query by product_id and version"
                        )
                        from primedata.db.models import ArtifactStatus, PipelineArtifact

                        fallback_artifacts = (
                            db.query(PipelineArtifact)
                            .filter(
                                PipelineArtifact.product_id == product_id,
                                PipelineArtifact.version == version,
                                PipelineArtifact.status != ArtifactStatus.PURGED,
                            )
                            .all()
                        )

                        if fallback_artifacts:
                            logger.info(
                                f"Found {len(fallback_artifacts)} artifacts via fallback query (product_id={product_id}, version={version})"
                            )
                            # Rebuild summary from fallback artifacts
                            artifact_summary = {
                                "total_artifacts": len(fallback_artifacts),
                                "total_size_bytes": sum(a.file_size for a in fallback_artifacts),
                                "by_stage": {},
                                "by_type": {},
                            }

                            for artifact in fallback_artifacts:
                                # By stage
                                if artifact.stage_name not in artifact_summary["by_stage"]:
                                    artifact_summary["by_stage"][artifact.stage_name] = {
                                        "count": 0,
                                        "total_size_bytes": 0,
                                        "artifacts": [],
                                    }
                                artifact_summary["by_stage"][artifact.stage_name]["count"] += 1
                                artifact_summary["by_stage"][artifact.stage_name]["total_size_bytes"] += artifact.file_size
                                artifact_summary["by_stage"][artifact.stage_name]["artifacts"].append(
                                    {
                                        "id": str(artifact.id),
                                        "name": artifact.artifact_name,
                                        "type": artifact.artifact_type.value,
                                        "size_bytes": artifact.file_size,
                                        "storage_key": artifact.storage_key,
                                    }
                                )

                                # By type
                                type_key = artifact.artifact_type.value
                                if type_key not in artifact_summary["by_type"]:
                                    artifact_summary["by_type"][type_key] = {"count": 0, "total_size_bytes": 0}
                                artifact_summary["by_type"][type_key]["count"] += 1
                                artifact_summary["by_type"][type_key]["total_size_bytes"] += artifact.file_size

                    logger.info(
                        f"Artifact summary query: pipeline_run_id={pipeline_run_uuid}, found {artifact_summary['total_artifacts']} artifacts"
                    )

                    # Update pipeline_runs.metrics with artifact summary
                    if pipeline_run.metrics is None:
                        pipeline_run.metrics = {}
                    pipeline_run.metrics["artifacts"] = artifact_summary
                    db.commit()

                    logger.info(
                        f"Added artifact summary to pipeline run metrics: {artifact_summary['total_artifacts']} artifacts, {artifact_summary['total_size_bytes']} bytes"
                    )
                    std_logger.info(
                        f"Added artifact summary to pipeline run metrics: {artifact_summary['total_artifacts']} artifacts, {artifact_summary['total_size_bytes']} bytes"
                    )
                else:
                    logger.warning(f"Pipeline run {pipeline_run_uuid} not found for artifact summary")
            except Exception as e:
                logger.error(f"Failed to add artifact summary to pipeline run: {e}", exc_info=True)
                std_logger.error(f"Failed to add artifact summary to pipeline run: {e}", exc_info=True)
                # Don't fail finalize if artifact summary fails

        result = {
            "status": "completed",
            "pipeline_duration": "complete",
            "product_status": "ready",
            "version": version,
            "files_finalized": len(raw_files_to_finalize),
            "final_message": f"Pipeline completed successfully for product {product_id}",
        }

        return result

    except Exception as e:
        logger.error(f"Finalization failed: {e}", exc_info=True)
        std_logger.error(f"Finalization failed: {e}", exc_info=True)

        # If finalize fails, mark all PROCESSING files as FAILED so they can be retried
        try:
            raw_files_in_error = (
                db.query(RawFile)
                .filter(
                    RawFile.product_id == product_id, RawFile.version == version, RawFile.status == RawFileStatus.PROCESSING
                )
                .all()
            )

            if raw_files_in_error:
                for record in raw_files_in_error:
                    record.status = RawFileStatus.FAILED
                    record.error_message = f"Pipeline finalization failed: {str(e)}"
                db.commit()
                logger.warning(f"Marked {len(raw_files_in_error)} raw files as FAILED due to finalization error")
                std_logger.warning(f"Marked {len(raw_files_in_error)} raw files as FAILED due to finalization error")
        except Exception as db_error:
            logger.error(f"Failed to update raw file status during finalization error: {db_error}")
            std_logger.error(f"Failed to update raw file status during finalization error: {db_error}")
            db.rollback()

        if pipeline_run_id:
            update_pipeline_status(pipeline_run_id, "failed", {"error": str(e)})
        raise
    finally:
        db.close()
