"""add_s3_path_columns_and_archiving_fields

Revision ID: 2c3c514d31f1
Revises: bda98fc65abe
Create Date: 2025-12-27 16:36:35.472433

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '2c3c514d31f1'
down_revision = 'bda98fc65abe'
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    
    # Add archiving fields to dq_violations
    op.add_column('dq_violations', sa.Column('archived_at', sa.DateTime(timezone=True), nullable=True))
    op.add_column('dq_violations', sa.Column('archived_to_s3', sa.Boolean(), nullable=False, server_default='false'))
    op.create_index('idx_dq_violations_product_version_severity', 'dq_violations', ['product_id', 'version', 'severity'], unique=False)
    
    # Fix pipeline_artifacts NOT NULL columns (set defaults for existing NULL values)
    op.execute("UPDATE pipeline_artifacts SET checksum = '' WHERE checksum IS NULL")
    op.execute("UPDATE pipeline_artifacts SET minio_etag = '' WHERE minio_etag IS NULL")
    op.alter_column('pipeline_artifacts', 'checksum',
               existing_type=sa.VARCHAR(length=64),
               nullable=False)
    op.alter_column('pipeline_artifacts', 'minio_etag',
               existing_type=sa.VARCHAR(length=255),
               nullable=False)
    
    # Add S3 path columns to pipeline_runs
    op.add_column('pipeline_runs', sa.Column('metrics_path', sa.String(length=1000), nullable=True))
    op.add_column('pipeline_runs', sa.Column('archived_at', sa.DateTime(timezone=True), nullable=True))
    
    # Fix pipeline_runs NOT NULL columns (set defaults for existing NULL values)
    op.execute("UPDATE pipeline_runs SET started_at = created_at WHERE started_at IS NULL")
    op.execute("UPDATE pipeline_runs SET dag_run_id = 'legacy_' || id::text WHERE dag_run_id IS NULL")
    op.alter_column('pipeline_runs', 'started_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=False)
    op.alter_column('pipeline_runs', 'dag_run_id',
               existing_type=sa.VARCHAR(length=255),
               nullable=False)
    op.create_index('idx_pipeline_runs_created_at', 'pipeline_runs', ['created_at'], unique=False)
    op.create_index('idx_pipeline_runs_product_status_created', 'pipeline_runs', ['product_id', 'status', 'created_at'], unique=False)
    
    # Add S3 path columns to products
    op.add_column('products', sa.Column('preprocessing_stats_path', sa.String(length=1000), nullable=True))
    op.add_column('products', sa.Column('readiness_fingerprint_path', sa.String(length=1000), nullable=True))
    op.add_column('products', sa.Column('chunk_metrics_path', sa.String(length=1000), nullable=True))
    op.create_index('idx_products_created_at', 'products', ['created_at'], unique=False)
    op.create_index('idx_products_status', 'products', ['status'], unique=False)
    
    # Fix raw_files NOT NULL columns (set defaults for existing NULL values)
    op.execute("UPDATE raw_files SET content_type = 'application/octet-stream' WHERE content_type IS NULL")
    op.execute("UPDATE raw_files SET file_checksum = '' WHERE file_checksum IS NULL")
    op.alter_column('raw_files', 'content_type',
               existing_type=sa.VARCHAR(length=255),
               nullable=False,
               server_default='application/octet-stream')
    op.alter_column('raw_files', 'file_checksum',
               existing_type=sa.VARCHAR(length=64),
               nullable=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('raw_files', 'file_checksum',
               existing_type=sa.VARCHAR(length=64),
               nullable=True)
    op.alter_column('raw_files', 'content_type',
               existing_type=sa.VARCHAR(length=255),
               nullable=True)
    op.drop_index('idx_products_status', table_name='products')
    op.drop_index('idx_products_created_at', table_name='products')
    op.drop_column('products', 'chunk_metrics_path')
    op.drop_column('products', 'readiness_fingerprint_path')
    op.drop_column('products', 'preprocessing_stats_path')
    op.drop_index('idx_pipeline_runs_product_status_created', table_name='pipeline_runs')
    op.drop_index('idx_pipeline_runs_created_at', table_name='pipeline_runs')
    op.alter_column('pipeline_runs', 'dag_run_id',
               existing_type=sa.VARCHAR(length=255),
               nullable=True)
    op.alter_column('pipeline_runs', 'started_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True)
    op.drop_column('pipeline_runs', 'archived_at')
    op.drop_column('pipeline_runs', 'metrics_path')
    op.alter_column('pipeline_artifacts', 'minio_etag',
               existing_type=sa.VARCHAR(length=255),
               nullable=True)
    op.alter_column('pipeline_artifacts', 'checksum',
               existing_type=sa.VARCHAR(length=64),
               nullable=True)
    op.drop_index('idx_dq_violations_product_version_severity', table_name='dq_violations')
    op.drop_column('dq_violations', 'archived_to_s3')
    op.drop_column('dq_violations', 'archived_at')
    # ### end Alembic commands ###
